{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "from cluster_analysis import *\n",
    "# from LOR_calculation import *\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mtb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species='Mycobacterium_tuberculosis'\n",
    "\n",
    "genome_ids_file=f\"../../pangenome-repo/Pangenome-Analysis-Workflow/genome_ids/{species}_genome_ids.csv\"\n",
    "clstr_freq_file=f\"../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}_cluster_frequencies.csv\"\n",
    "clstr_file=f\"../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}.fasta.clstr\"\n",
    "clstr_fasta_file=f\"../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}.fasta\"\n",
    "\n",
    "samples_df = pd.read_csv(genome_ids_file, dtype=str)\n",
    "samples_list=samples_df['genome.genome_ids'].tolist()\n",
    "\n",
    "clstr_freq_df=pd.read_csv(clstr_freq_file, index_col=1)\n",
    "clstr_freq_df=clstr_freq_df.drop(clstr_freq_df.columns[0], axis=1)\n",
    "clstr_list=clstr_freq_df.index.tolist()\n",
    "\n",
    "df = pd.DataFrame(index=samples_list, columns=clstr_list)\n",
    "df = df.fillna(0)\n",
    "\n",
    "with open(clstr_file) as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\">Cluster\"):\n",
    "            #if its a > line get the cluster id and start counting its occurences in which samples by saving it to the local var\n",
    "            cluster_id=line[1:].strip()\n",
    "        else:\n",
    "            #else its a line designating the sample genome, one that the last cluster_id is present in\n",
    "            # get the 1st group matching here \\d\\t\\d+aa, >fig\\|([\\d\\.\\d]+).+\n",
    "\n",
    "            genome_id=re.match(r\"\\d+\\t\\d+aa, >fig\\|([\\d\\.\\d]+).+\", line).group(1)\n",
    "            genome_id=genome_id[:-1]  #removing the trailing .\n",
    "            df.loc[genome_id, cluster_id]+=1\n",
    "\n",
    "df = df.T\n",
    "df.to_csv(f'../../data/presence_matrices/{species}_GxS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species='Mycobacterium_tuberculosis'; drug=''\n",
    "\n",
    "presence_df = pd.read_csv(f'../../data/presence_matrices/{species}_GxS.csv', index_col=0); \n",
    "X_df=presence_df.T\n",
    "X_df.index = X_df.index.astype('float')\n",
    "# X_df.index = X_df.index.astype('object')\n",
    "\n",
    "\n",
    "pheno_df= pd.read_csv(f'../../data/phenotypes/{species}_{drug}.csv', index_col=0)\n",
    "y_df=pheno_df\n",
    "y_df.index = y_df.index.astype('float')\n",
    "\n",
    "for gene in X_df.columns:\n",
    "    \n",
    "    if X_df[gene].std() == 0:\n",
    "        X_df.drop(gene, axis=1, inplace=True)\n",
    "\n",
    "X_df = X_df.sort_index()\n",
    "y_df = y_df.sort_index()\n",
    "\n",
    "\n",
    "y_indices=list(y_df.index)\n",
    "X_indices=list(X_df.index)\n",
    "\n",
    "intersection = [i for i in y_indices if i in X_indices]\n",
    "y_df = y_df.loc[intersection]\n",
    "X_df = X_df.loc[intersection]\n",
    "\n",
    "# -- removing duplicate rows? --          to check th e nature of duplicates it could be an error through type conversion\n",
    "X_df = X_df.loc[~X_df.index.duplicated(keep='first')]\n",
    "\n",
    "X = X_df.values\n",
    "y = y_df.values\n",
    "\n",
    "labeled_matrix = pd.concat([X_df, y_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E coli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "species='Escherichia_coli'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# species='Escherichia_coli'\n",
    "\n",
    "# genome_ids_file=f\"../../pangenome-repo/Pangenome-Analysis-Workflow/genome_ids/{species}_genome_ids.csv\"\n",
    "# clstr_freq_file=f\"../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}_cluster_frequencies.csv\"\n",
    "# clstr_file=f\"../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}.fasta.clstr\"\n",
    "# clstr_fasta_file=f\"../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}.fasta\"\n",
    "\n",
    "# samples_df = pd.read_csv(genome_ids_file, dtype=str)\n",
    "# samples_list=samples_df['genome.genome_ids'].tolist()\n",
    "\n",
    "# clstr_freq_df=pd.read_csv(clstr_freq_file, index_col=1)\n",
    "# clstr_freq_df=clstr_freq_df.drop(clstr_freq_df.columns[0], axis=1)\n",
    "# clstr_list=clstr_freq_df.index.tolist()\n",
    "\n",
    "# df = pd.DataFrame(index=samples_list, columns=clstr_list)\n",
    "# df = df.fillna(0)\n",
    "\n",
    "# with open(clstr_file) as f:\n",
    "#     for line in f:\n",
    "#         if line.startswith(\">Cluster\"):\n",
    "#             #if its a > line get the cluster id and start counting its occurences in which samples by saving it to the local var\n",
    "#             cluster_id=line[1:].strip()\n",
    "#         else:\n",
    "#             #else its a line designating the sample genome, one that the last cluster_id is present in\n",
    "#             # get the 1st group matching here \\d\\t\\d+aa, >fig\\|([\\d\\.\\d]+).+\n",
    "\n",
    "#             genome_id=re.match(r\"\\d+\\t\\d+aa, >fig\\|([\\d\\.\\d]+).+\", line).group(1)\n",
    "#             genome_id=genome_id[:-1]  #removing the trailing .\n",
    "#             df.loc[genome_id, cluster_id]+=1\n",
    "\n",
    "# df = df.T\n",
    "# df.to_csv(f'../../data/presence_matrices/{species}_GxS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filtration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unknowns and hypotheticals and freq < 0.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clstr_df= get_cluster_representatives(f'../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}.fasta.clstr')\n",
    "_product_df=get_representative_products(f'../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}.fasta')\n",
    "product_df = combine_cluster_product(clstr_df, _product_df)\n",
    "freq_df = get_cluster_frequency(f'../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}_cluster_frequencies.csv')\n",
    "\n",
    "#  --counting those thar have hypotehtical in the product name\n",
    "product_df['hypothetical'] = product_df['product_name'].str.contains('hypothetical', case=False)\n",
    "product_df['hypothetical'].sum()\n",
    "\n",
    "#  --same for unknown\n",
    "product_df['unknown'] = product_df['product_name'].str.contains('unknown', case=False)\n",
    "product_df['unknown'].sum()\n",
    "\n",
    "# now will get a list of clusters that are either hypo or unknown to filter out\n",
    "hypothetical_clusters = product_df[product_df['hypothetical']==True].index.tolist()\n",
    "unknown_clusters = product_df[product_df['unknown']==True].index.tolist()\n",
    "\n",
    "to_remove = list(set(hypothetical_clusters).union(unknown_clusters))\n",
    "\n",
    "# -- get the list of clusters that are present in less than 10 genomes\n",
    "low_freq_clusters = freq_df[freq_df['frequency']<10].index.tolist()\n",
    "\n",
    "to_remove = list(set(to_remove).union(low_freq_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m low_freq_clusters \u001b[38;5;241m=\u001b[39m freq_df[freq_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrequency\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m10\u001b[39m]\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     23\u001b[0m to_remove \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(to_remove)\u001b[38;5;241m.\u001b[39munion(low_freq_clusters))\n\u001b[0;32m---> 25\u001b[0m presence_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../data/presence_matrices/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mspecies\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_GxS.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m; \n\u001b[1;32m     26\u001b[0m X_df\u001b[38;5;241m=\u001b[39mpresence_df\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     27\u001b[0m X_df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m X_df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:232\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    230\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_concatenate_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:365\u001b[0m, in \u001b[0;36m_concatenate_chunks\u001b[0;34m(chunks)\u001b[0m\n\u001b[1;32m    363\u001b[0m arrs \u001b[38;5;241m=\u001b[39m [chunk\u001b[38;5;241m.\u001b[39mpop(name) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# Check each arr for consistent types.\u001b[39;00m\n\u001b[0;32m--> 365\u001b[0m dtypes \u001b[38;5;241m=\u001b[39m {a\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrs}\n\u001b[1;32m    366\u001b[0m \u001b[38;5;66;03m# TODO: shouldn't we exclude all EA dtypes here?\u001b[39;00m\n\u001b[1;32m    367\u001b[0m numpy_dtypes \u001b[38;5;241m=\u001b[39m {x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m dtypes \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_categorical_dtype(x)}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "presence_df = pd.read_csv(f'../../data/presence_matrices/{species}_GxS.csv', index_col=0); \n",
    "X_df=presence_df.T\n",
    "X_df.index = X_df.index.astype('float')\n",
    "# X_df.index = X_df.index.astype('object')\n",
    "\n",
    "# -- removing hypothetical and unknown clusters\n",
    "X_df = X_df.drop(to_remove, axis=1)\n",
    "\n",
    "# -- memoized for later use if needed\n",
    "# X_df.to_csv(f'../../data/temp/X_{species}_GxS_filtered_hypo_unknowns_freq10.csv')\n",
    "# X_df = pd.read_csv(f'../../data/temp/X_{species}_SxG_filtered_hypo_unknowns.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.read_csv(f'../../data/temp/X_{species}_GxS_filtered_hypo_unknowns_freq10.csv', index_col=0)\n",
    "\n",
    "# -- fix, SxG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  -- removing genes belowe a variance threshold\n",
    "v_t=0.1\n",
    "\n",
    "for gene in X_df.columns:\n",
    "    if X_df[gene].var() < v_t: \n",
    "        X_df.drop(gene, axis=1, inplace=True)\n",
    "\n",
    "X_df.to_csv(f'../../data/temp/X_{species}_SxG_filtered_var_{v_t}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species='Escherichia_coli'; drug=''; v_t=0.1\n",
    "\n",
    "X_df = pd.read_csv(f'../../data/temp/X_{species}_SxG_filtered_var_{v_t}.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## labels - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimethoprim = ['folA','dfrA','dfrG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs = ['streptomycin',\n",
    "        'sulfamethoxazole',\n",
    "        'tetracycline',\n",
    "        'cefalothin',\n",
    "        'trimethoprim_sulphamethoxazole',\n",
    "        'trimethoprim',\n",
    "        'amoxicillin',\n",
    "        'ampicillin',\n",
    "        'doripenem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499, 13689)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug='trimethoprim'\n",
    "\n",
    "pheno_df= pd.read_csv(f'../../metadata/{species}/{species}_{drug}.csv', index_col=0)\n",
    "y_df=pheno_df\n",
    "y_df.index = y_df.index.astype('float')\n",
    "\n",
    "y_df = y_df.sort_index()\n",
    "\n",
    "y_indices=list(y_df.index)\n",
    "\n",
    "\n",
    "X_df = X_df.sort_index()\n",
    "y_df = y_df.sort_index()\n",
    "\n",
    "y_indices=list(y_df.index)\n",
    "X_indices=list(X_df.index)\n",
    "\n",
    "intersection = [i for i in y_indices if i in X_indices]\n",
    "y_df = y_df.loc[intersection]\n",
    "X_df = X_df.loc[intersection]\n",
    "\n",
    "X_df = X_df.sort_index()\n",
    "y_df = y_df.sort_index() # -- just making sure bcs im paranoid\n",
    "\n",
    "X = X_df.values\n",
    "y = y_df.values\n",
    "\n",
    "\n",
    "\n",
    "labeled_matrix = pd.concat([X_df, y_df], axis=1)\n",
    "labeled_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dihydrofolate reductase (EC 1.5.1.3) for folA\n",
    "\n",
    "product_df = product_df.drop(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  -- dont forget to add this to a module (maybe cluster analysis)\n",
    "def transform_cluster_to_product(cluster_name, clstr_product_df):\n",
    "    #match it ffrom clstr_poduct_df from the index\n",
    "    cluster = clstr_product_df.loc[cluster_name]\n",
    "    product=cluster['product_name']\n",
    "    \n",
    "    return product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to perform different statistical and machine learning approaches to associate each feature (gene) with the phenotype of interest (one drug at a time).  _The phenotype of interest is the resistance to a specific drug._  \n",
    "Would lated take the top 100 genes of each approach and compare how 1. if the known ARGs are ranked high in each and 2. if there is enough overlap. Would probably combine all to create a pool of genes to test for interaction later on.\n",
    "\n",
    "Ideas on statistical approaches:  \n",
    "- Mutual Information (MI)\n",
    "- Fisher's exact test/Chi-squared test\n",
    "- ANOVA\n",
    "- Jaccard index\n",
    "\n",
    "Ideas on machine learning approaches:\n",
    "- Random Forest\n",
    "- SVM\n",
    "- Logistic Regression\n",
    "- Gradient Boosting\n",
    "- Neural Networks\n",
    "\n",
    "Would probably start with ensemble methods, using weights sum/mean as the final score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ARGs(ARG_products:pd.DataFrame, top_genes:pd.DataFrame):\n",
    "    '''\n",
    "    Takes in a list of ARG products and a list of top genes and returns a dict of ARGs that are in the top genes (with their rank)\n",
    "    '''\n",
    "    ARGs = {}\n",
    "    for gene in top_genes:\n",
    "        for product in ARG_products:\n",
    "            if product in gene:\n",
    "                ARGs[top_genes.index(gene)]=gene\n",
    "    return ARGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- computing MI between each col and the last col in labeled_matrix\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "mi_scores = {}\n",
    "for col in labeled_matrix.columns[:-1]:\n",
    "    mi_scores[col]=mutual_info_score(labeled_matrix[col], labeled_matrix['SIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting top 100 scores\n",
    "\n",
    "MI_top_100 = sorted(mi_scores, key=mi_scores.get, reverse=True)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in  range(len(MI_top_100)):\n",
    "    MI_top_100[i]=transform_cluster_to_product(MI_top_100[i], product_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MI_top_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{20: 'Dihydrofolate reductase (EC 1.5.1.3)_1'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- checking if tetramethoprim are there\n",
    "trimethoprim = ['folA','dfrA','dfrG']\n",
    "trimethoprim_products = ['Dihydrofolate reductase (EC 1.5.1.3)','trimethoprim-resistant dihydrofolate reductase DfrA1','Dihydrofolate reductase (EC 1.5.1.3)']\n",
    "\n",
    "parse_ARGs(trimethoprim_products, MI_top_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to detect interactions between the pool of genes we now have, also following statictical and machine learning approaches.\n",
    "\n",
    "Statistical approaches:\n",
    "- 2 way ANOVA\n",
    "- correlation (rho) t test (this should be coupled with a ML ensembl)\n",
    "\n",
    "Machine learning approaches:\n",
    "- Logistic Regression with interaction terms\n",
    "- SVM ensemble\n",
    "Following the assumption in Kavvas et al. (2018), we can use SVM and consider high correlation between features as a sign of interaction.\n",
    "- Neural Networks\n",
    "They mentioned that it would be nice to consider other models in other works. I think NN would be a good candidate because, even though it non-interpretable, it can capture complex relationship between features, if we can find a way to correlate the weights with the features. For this purpose, we might consider \n",
    "\n",
    "_For computational complexity, considering the interactions to test for significance our of the SVMs correlation_\n",
    "\n",
    "\n",
    "**ALWAYS ACCOUNTING FOR MULTIPLE TESTING USING THE ENJAMINI-HOCHBERG CORRECTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 200\n",
    "\n",
    "\n",
    "models=[]\n",
    "for i in range(n_models):\n",
    "    X_boot, y_boot = resample(X, y, random_state=i)\n",
    "    model=SGDClassifier(loss='hinge', penalty= 'l1', max_iter=1000, tol=1e-3)\n",
    "    model.fit(X_boot, y_boot.ravel())\n",
    "    models.append(model)\n",
    "\n",
    "weights=np.zeros((n_models, X.shape[1]))\n",
    "for i in range(n_models):\n",
    "    weights[i]=models[i].coef_\n",
    "weights_df = pd.DataFrame(weights, columns=X_df.columns)\n",
    "\n",
    "corr_SVM = weights_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it in a csv\n",
    "corr_SVM.to_csv(f'../../data/temp/corr_SVM_{species}_{drug}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_SVM = pd.read_csv(f'../../data/temp/corr_SVM_{species}_{drug}.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0.2 # -- UNSIGNED\n",
    "\n",
    "G=nx.Graph()\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "\n",
    "    for j in range(i+1, X.shape[1]):\n",
    "        node_i=X_df.columns[i]; node_j=X_df.columns[j]\n",
    "        correlation = corr_SVM.iloc[i, j]\n",
    "        if correlation > t:\n",
    "            G.add_edge(node_i, node_j, weight=correlation)\n",
    "\n",
    "# G=set_cluster_attributes(G)\n",
    "# nx.set_node_attributes(G, log_odds_dict, 'log_odds')\n",
    "\n",
    "nx.write_graphml(G, f'../../data/nets/{species}/{n_models}_randomized_SVM_{t}unsigned_corr_{drug}.graphml')\n",
    "\n",
    "edges_list = list(G.edges)\n",
    "edge_weights = [G[u][v]['weight'] for u, v in edges_list]\n",
    "print(f\"E: {len(edges_list)}, V: {G.number_of_nodes()} when {n_models} models are used, with corr threshold={t}; species={species}, drug={drug}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: 7554, V: 5679 when 200 models are used, with corr threshold=0.2; species=Escherichia_coli, drug=trimethoprim.\n"
     ]
    }
   ],
   "source": [
    "# nx.write_graphml(G, f'../../data/nets/{species}/{n_models}_randomized_SVM_{t}unsigned_corr_{drug}.graphml')\n",
    "\n",
    "# edges_list = list(G.edges)\n",
    "# edge_weights = [G[u][v]['weight'] for u, v in edges_list]\n",
    "# print(f\"E: {len(edges_list)}, V: {G.number_of_nodes()} when {n_models} models are used, with corr threshold={t}; species={species}, drug={drug}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0; insig=0\n",
    "save_edges=[]\n",
    "p_values_list=[]\n",
    "\n",
    "for pair in edges_list:\n",
    "    a, b = pair\n",
    "    print(\" --- looking into the edge between\", a, \"and\", b)\n",
    "\n",
    "    X_selected = X_df.loc[:, [a, b]]\n",
    "\n",
    "    print(X_selected.shape)\n",
    "\n",
    "    X_selected = sm.add_constant(X_selected)\n",
    "    print(X_selected.shape)\n",
    "    if X_selected.shape[1]==2:\n",
    "        X_selected['interaction'] = X_selected.iloc[:, 1] * X_selected.iloc[:, 0]\n",
    "    X_selected['interaction'] = X_selected.iloc[:, 1] * X_selected.iloc[:, 2]\n",
    "\n",
    "    print(X_selected.shape)\n",
    "    \n",
    "    col=False\n",
    "\n",
    "\n",
    "    #account for collinearity\n",
    "    # --- matrix way\n",
    "    if np.linalg.matrix_rank(X_selected) < X_selected.shape[1]:\n",
    "        print('collinearity; deleted')\n",
    "        # G.remove_edge(*pair)\n",
    "        continue\n",
    "\n",
    "    # --- paper way\n",
    "    # if (X_selected['interaction']==X_selected[a]).all() or (X_selected['interaction']==X_selected[b]).all():      \n",
    "    #     col=True  \n",
    "    #     print(f'colinearity between interaction and one of the features {a} or {b}; deleted')\n",
    "    # elif (X_selected[b]==X_selected[a]).all():\n",
    "    #     col=True\n",
    "    #     print(f'colinearity between {a} and {b}; deleted')\n",
    "    # elif (X_selected['const']==X_selected[a]).all() or (X_selected['const']==X_selected[b]).all() or (X_selected['const']==X_selected['interaction']).all():\n",
    "    #     col=True\n",
    "    #     print('colinearity between one of the features and the constant term?!')\n",
    "\n",
    "    # if col:\n",
    "    #     G.remove_edge(*pair)\n",
    "    #     continue\n",
    "\n",
    "    # --- logistic regression\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            logit_model = sm.Logit(y, X_selected)\n",
    "            result = logit_model.fit()\n",
    "    except:\n",
    "        print('singular matrix; deleted')\n",
    "\n",
    "    p_values_list.append(p_values['interaction'])\n",
    "\n",
    "    # coefficients = result.params\n",
    "    p_values = result.pvalues\n",
    "\n",
    "    if p_values['interaction']<0.05:\n",
    "        print('not significant; deleted')\n",
    "        # G.remove_edge(*pair)\n",
    "        insig+=1\n",
    "    else:\n",
    "        count+=1\n",
    "        save_edges.append(pair)\n",
    "\n",
    "\n",
    "    # --- multiple testing Benjamini-Hochberg correction\n",
    "    reject, corrected_p_values= multipletests(p_values_list, method='fdr_bh')\n",
    "\n",
    "    for i, pair in enumerate(edges_list):\n",
    "        if corrected_p_values[i] < 0.05:\n",
    "            print(f'Edge {pair} is not significant; deleted')\n",
    "            G.remove_edge(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-way ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_list = list(G.edges)\n",
    "edge_weights = [G[u][v]['weight'] for u, v in edges_list]\n",
    "print(f\"E: {len(edges_list)}, V: {G.number_of_nodes()} when {n_models} models are used, with corr threshold={t}; species={species}, drug={drug}.\")\n",
    "\n",
    "save_edges=[]\n",
    "\n",
    "edges_list_2=[(i.replace(' ', '_'), j.replace(' ', '_')) for i, j in edges_list]\n",
    "labeled_matrix_2 = labeled_matrix.rename(columns={col: col.replace(' ', '_') for col in labeled_matrix.columns})\n",
    "p_values={}\n",
    "\n",
    "count_nan=0\n",
    "save_nan_testing=[]\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for pair in edges_list_2:\n",
    "        a, b = pair\n",
    "        # print(\" --- looking into the interaction between\", a, \"and\", b)\n",
    "        formula = f'SIR ~ {a} + {b} + {a}:{b}'\n",
    "        model = ols(formula, data=labeled_matrix_2).fit()\n",
    "        anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "        if not np.isnan(anova_table['PR(>F)'][2]):\n",
    "            p_values[anova_table.index[2]]=anova_table['PR(>F)'][2]\n",
    "        else:\n",
    "            save_nan_testing.append(pair)\n",
    "            count_nan+=1\n",
    "        \n",
    "print(f\"{count_nan} interactions have nan p-values - removed\")\n",
    "\n",
    "# mst correction - using fdr\n",
    "\n",
    "gene_gene_pair = list(p_values.keys())\n",
    "p_values_list = list(p_values.values())\n",
    "\n",
    "reject, corrected_p_values= fdrcorrection(p_values_list)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'gene_gene': gene_gene_pair,\n",
    "    'p-value': p_values_list,\n",
    "    'corr_p_value': corrected_p_values,\n",
    "    'reject_H0': reject\n",
    "})\n",
    "\n",
    "count_rejected=0\n",
    "for i in reject:\n",
    "    if i:\n",
    "        count_rejected+=1\n",
    "\n",
    "print(f\"Number of rejected H0: {count_rejected} out of {len(reject)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clstr_df= get_cluster_representatives(f'../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}.fasta.clstr')\n",
    "_product_df=get_representative_products(f'../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}.fasta')\n",
    "product_df = combine_cluster_product(clstr_df, _product_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  left joing X_df and product_df on the index\n",
    "X_t= X_df.T\n",
    "test = X_t.join(product_df, how='left')\n",
    "l=test['product_name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Cluster'] = test.index\n",
    "test.index = test['product_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_product_df.index= _product_df['product_name']\n",
    "p_ =_product_df.drop('product_name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner join test and _product_df on the index\n",
    "test = test.join(_product_df, how='inner', lsuffix='_caller', rsuffix='_other')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['product_name'].to_list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
