{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "from cluster_analysis import *\n",
    "# from LOR_calculation import *\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mtb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species='Mycobacterium_tuberculosis'\n",
    "\n",
    "genome_ids_file=f\"../../pangenome-repo/Pangenome-Analysis-Workflow/genome_ids/{species}_genome_ids.csv\"\n",
    "clstr_freq_file=f\"../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}_cluster_frequencies.csv\"\n",
    "clstr_file=f\"../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}.fasta.clstr\"\n",
    "clstr_fasta_file=f\"../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}.fasta\"\n",
    "\n",
    "samples_df = pd.read_csv(genome_ids_file, dtype=str)\n",
    "samples_list=samples_df['genome.genome_ids'].tolist()\n",
    "\n",
    "clstr_freq_df=pd.read_csv(clstr_freq_file, index_col=1)\n",
    "clstr_freq_df=clstr_freq_df.drop(clstr_freq_df.columns[0], axis=1)\n",
    "clstr_list=clstr_freq_df.index.tolist()\n",
    "\n",
    "df = pd.DataFrame(index=samples_list, columns=clstr_list)\n",
    "df = df.fillna(0)\n",
    "\n",
    "with open(clstr_file) as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\">Cluster\"):\n",
    "            #if its a > line get the cluster id and start counting its occurences in which samples by saving it to the local var\n",
    "            cluster_id=line[1:].strip()\n",
    "        else:\n",
    "            #else its a line designating the sample genome, one that the last cluster_id is present in\n",
    "            # get the 1st group matching here \\d\\t\\d+aa, >fig\\|([\\d\\.\\d]+).+\n",
    "\n",
    "            genome_id=re.match(r\"\\d+\\t\\d+aa, >fig\\|([\\d\\.\\d]+).+\", line).group(1)\n",
    "            genome_id=genome_id[:-1]  #removing the trailing .\n",
    "            df.loc[genome_id, cluster_id]+=1\n",
    "\n",
    "df = df.T\n",
    "df.to_csv(f'../../data/presence_matrices/{species}_GxS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species='Mycobacterium_tuberculosis'; drug=''\n",
    "\n",
    "presence_df = pd.read_csv(f'../../data/presence_matrices/{species}_GxS.csv', index_col=0); \n",
    "X_df=presence_df.T\n",
    "X_df.index = X_df.index.astype('float')\n",
    "# X_df.index = X_df.index.astype('object')\n",
    "\n",
    "\n",
    "pheno_df= pd.read_csv(f'../../data/phenotypes/{species}_{drug}.csv', index_col=0)\n",
    "y_df=pheno_df\n",
    "y_df.index = y_df.index.astype('float')\n",
    "\n",
    "for gene in X_df.columns:\n",
    "    \n",
    "    if X_df[gene].std() == 0:\n",
    "        X_df.drop(gene, axis=1, inplace=True)\n",
    "\n",
    "X_df = X_df.sort_index()\n",
    "y_df = y_df.sort_index()\n",
    "\n",
    "\n",
    "y_indices=list(y_df.index)\n",
    "X_indices=list(X_df.index)\n",
    "\n",
    "intersection = [i for i in y_indices if i in X_indices]\n",
    "y_df = y_df.loc[intersection]\n",
    "X_df = X_df.loc[intersection]\n",
    "\n",
    "# -- removing duplicate rows? --          to check th e nature of duplicates it could be an error through type conversion\n",
    "X_df = X_df.loc[~X_df.index.duplicated(keep='first')]\n",
    "\n",
    "X = X_df.values\n",
    "y = y_df.values\n",
    "\n",
    "labeled_matrix = pd.concat([X_df, y_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E coli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "species='Escherichia_coli'\n",
    "\n",
    "X_df = pd.read_csv(f'../../data/temp/X_{species}_SxG_filtered_hypo_unknowns_freq5.csv', index_col=0) # filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# species='Escherichia_coli'\n",
    "\n",
    "# genome_ids_file=f\"../../pangenome-repo/Pangenome-Analysis-Workflow/genome_ids/{species}_genome_ids.csv\"\n",
    "# clstr_freq_file=f\"../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}_cluster_frequencies.csv\"\n",
    "# clstr_file=f\"../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}.fasta.clstr\"\n",
    "# clstr_fasta_file=f\"../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}.fasta\"\n",
    "\n",
    "# samples_df = pd.read_csv(genome_ids_file, dtype=str)\n",
    "# samples_list=samples_df['genome.genome_ids'].tolist()\n",
    "\n",
    "# clstr_freq_df=pd.read_csv(clstr_freq_file, index_col=1)\n",
    "# clstr_freq_df=clstr_freq_df.drop(clstr_freq_df.columns[0], axis=1)\n",
    "# clstr_list=clstr_freq_df.index.tolist()\n",
    "\n",
    "# df = pd.DataFrame(index=samples_list, columns=clstr_list)\n",
    "# df = df.fillna(0)\n",
    "\n",
    "# with open(clstr_file) as f:\n",
    "#     for line in f:\n",
    "#         if line.startswith(\">Cluster\"):\n",
    "#             #if its a > line get the cluster id and start counting its occurences in which samples by saving it to the local var\n",
    "#             cluster_id=line[1:].strip()\n",
    "#         else:\n",
    "#             #else its a line designating the sample genome, one that the last cluster_id is present in\n",
    "#             # get the 1st group matching here \\d\\t\\d+aa, >fig\\|([\\d\\.\\d]+).+\n",
    "\n",
    "#             genome_id=re.match(r\"\\d+\\t\\d+aa, >fig\\|([\\d\\.\\d]+).+\", line).group(1)\n",
    "#             genome_id=genome_id[:-1]  #removing the trailing .\n",
    "#             df.loc[genome_id, cluster_id]+=1\n",
    "\n",
    "# df = df.T\n",
    "# df.to_csv(f'../../data/presence_matrices/{species}_GxS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filtration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unknowns and hypotheticals and freq < 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clstr_df= get_cluster_representatives(f'../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}.fasta.clstr')\n",
    "_product_df=get_representative_products(f'../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}.fasta')\n",
    "product_df = combine_cluster_product(clstr_df, _product_df)\n",
    "freq_df = get_cluster_frequency(f'../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}_cluster_frequencies.csv')\n",
    "\n",
    "#  --counting those thar have hypotehtical in the product name\n",
    "product_df['hypothetical'] = product_df['product_name'].str.contains('hypothetical', case=False)\n",
    "product_df['hypothetical'].sum()\n",
    "\n",
    "#  --same for unknown\n",
    "product_df['unknown'] = product_df['product_name'].str.contains('unknown', case=False)\n",
    "product_df['unknown'].sum()\n",
    "\n",
    "# now will get a list of clusters that are either hypo or unknown to filter out\n",
    "hypothetical_clusters = product_df[product_df['hypothetical']==True].index.tolist()\n",
    "unknown_clusters = product_df[product_df['unknown']==True].index.tolist()\n",
    "\n",
    "to_remove = list(set(hypothetical_clusters).union(unknown_clusters))\n",
    "\n",
    "# -- get the list of clusters that are present in less than 5 genomes\n",
    "low_freq_clusters = freq_df[freq_df['frequency']<5].index.tolist()\n",
    "\n",
    "to_remove = list(set(to_remove).union(low_freq_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presence_df = pd.read_csv(f'../../data/presence_matrices/{species}_GxS.csv', index_col=0); \n",
    "X_df=presence_df.T\n",
    "X_df.index = X_df.index.astype('float')\n",
    "# X_df.index = X_df.index.astype('object')\n",
    "\n",
    "# -- removing hypothetical and unknown clusters nad those present in lesss than 5 samples\n",
    "X_df = X_df.drop(to_remove, axis=1)\n",
    "\n",
    "# -- memoized for later use if needed\n",
    "X_df.to_csv(f'../../data/temp/X_{species}_SxG_filtered_hypo_unknowns_freq5.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  -- removing genes belowe a variance threshold\n",
    "v_t=0.1\n",
    "\n",
    "for gene in X_df.columns:\n",
    "    if X_df[gene].var() < v_t: \n",
    "        X_df.drop(gene, axis=1, inplace=True)\n",
    "\n",
    "X_df.to_csv(f'../../data/temp/X_{species}_SxG_filtered_var_{v_t}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species='Escherichia_coli'; drug=''; v_t=0.1\n",
    "\n",
    "X_df = pd.read_csv(f'../../data/temp/X_{species}_SxG_filtered_var_{v_t}.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## labels - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimethoprim = ['folA','dfrA','dfrG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs = ['streptomycin',\n",
    " 'sulfamethoxazole',\n",
    " 'tetracycline',\n",
    " 'cefalothin',\n",
    " 'trimethoprim_sulphamethoxazole',\n",
    " 'amoxicillin_clavulanic_acid',\n",
    " 'trimethoprim',\n",
    " 'amoxicillin',\n",
    " 'ampicillin',\n",
    " 'doripenem',\n",
    " 'levofloxacin',\n",
    " 'ciprofloxacin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1672, 18876)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug='ciprofloxacin'\n",
    "\n",
    "pheno_df= pd.read_csv(f'../../metadata/{species}/{species}_{drug}.csv', index_col=0)\n",
    "y_df=pheno_df\n",
    "y_df.index = y_df.index.astype('float')\n",
    "\n",
    "y_df = y_df.sort_index()\n",
    "\n",
    "y_indices=list(y_df.index)\n",
    "\n",
    "\n",
    "X_df = X_df.sort_index()\n",
    "y_df = y_df.sort_index()\n",
    "\n",
    "y_indices=list(y_df.index)\n",
    "X_indices=list(X_df.index)\n",
    "\n",
    "intersection = [i for i in y_indices if i in X_indices]\n",
    "y_df = y_df.loc[intersection]\n",
    "X_df = X_df.loc[intersection]\n",
    "\n",
    "X_df = X_df.sort_index()\n",
    "y_df = y_df.sort_index() # -- just making sure bcs im paranoid\n",
    "\n",
    "X = X_df.values\n",
    "y = y_df.values\n",
    "\n",
    "\n",
    "\n",
    "labeled_matrix = pd.concat([X_df, y_df], axis=1)\n",
    "labeled_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dihydrofolate reductase (EC 1.5.1.3) for folA\n",
    "\n",
    "product_df = product_df.drop(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  -- dont forget to add this to a module (maybe cluster analysis)\n",
    "def transform_cluster_to_product(cluster_name, clstr_product_df):\n",
    "    #match it ffrom clstr_poduct_df from the index\n",
    "    cluster = clstr_product_df.loc[cluster_name]\n",
    "    product=cluster['product_name']\n",
    "    \n",
    "    return product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to perform different statistical and machine learning approaches to associate each feature (gene) with the phenotype of interest (one drug at a time).  _The phenotype of interest is the resistance to a specific drug._  \n",
    "Would lated take the top 100 genes of each approach and compare how 1. if the known ARGs are ranked high in each and 2. if there is enough overlap. Would probably combine all to create a pool of genes to test for interaction later on.\n",
    "\n",
    "Ideas on statistical approaches:  \n",
    "- Mutual Information (MI)\n",
    "- Fisher's exact test/Chi-squared test\n",
    "- ANOVA\n",
    "- Jaccard index\n",
    "\n",
    "Ideas on machine learning approaches:\n",
    "- Random Forest\n",
    "- SVM\n",
    "- Logistic Regression\n",
    "- Gradient Boosting\n",
    "- Neural Networks\n",
    "\n",
    "Would probably start with ensemble methods, using weights sum/mean as the final score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ARGs(ARG_products:pd.DataFrame, top_genes:pd.DataFrame):\n",
    "    '''\n",
    "    Takes in a list of ARG products and a list of top genes and returns a dict of ARGs that are in the top genes (with their rank)\n",
    "    '''\n",
    "    ARGs = {}\n",
    "    for gene in top_genes:\n",
    "        for product in ARG_products:\n",
    "            if product in gene:\n",
    "                ARGs[top_genes.index(gene)]=gene\n",
    "    return ARGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- computing MI between each col and the last col in labeled_matrix\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "mi_scores = {}\n",
    "for col in labeled_matrix.columns[:-1]:\n",
    "    mi_scores[col]=mutual_info_score(labeled_matrix[col], labeled_matrix['SIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting top 100 scores\n",
    "\n",
    "MI_top_100 = sorted(mi_scores, key=mi_scores.get, reverse=True)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in  range(len(MI_top_100)):\n",
    "    MI_top_100[i]=transform_cluster_to_product(MI_top_100[i], product_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MI_top_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- checking if tetramethoprim are there\n",
    "trimethoprim = ['folA','dfrA','dfrG']\n",
    "trimethoprim_products = ['Dihydrofolate reductase (EC 1.5.1.3)','trimethoprim-resistant dihydrofolate reductase DfrA1','Dihydrofolate reductase (EC 1.5.1.3)']\n",
    "\n",
    "parse_ARGs(trimethoprim_products, MI_top_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to detect interactions between the pool of genes we now have, also following statictical and machine learning approaches.\n",
    "\n",
    "Statistical approaches:\n",
    "- 2 way ANOVA\n",
    "- correlation (rho) t test (this should be coupled with a ML ensembl)\n",
    "\n",
    "Machine learning approaches:\n",
    "- Logistic Regression with interaction terms\n",
    "- SVM ensemble\n",
    "Following the assumption in Kavvas et al. (2018), we can use SVM and consider high correlation between features as a sign of interaction.\n",
    "- Neural Networks\n",
    "They mentioned that it would be nice to consider other models in other works. I think NN would be a good candidate because, even though it non-interpretable, it can capture complex relationship between features, if we can find a way to correlate the weights with the features. For this purpose, we might consider \n",
    "\n",
    "_For computational complexity, considering the interactions to test for significance our of the SVMs correlation_\n",
    "\n",
    "\n",
    "**ALWAYS ACCOUNTING FOR MULTIPLE TESTING USING THE ENJAMINI-HOCHBERG CORRECTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-01 13:47:21.649043: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-01 13:47:21.650456: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-01 13:47:21.708302: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-01 13:47:21.916225: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-01 13:47:22.718910: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rayane/miniconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5633 - loss: 2.8547\n",
      "Epoch 2/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7470 - loss: 1.3594\n",
      "Epoch 3/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8555 - loss: 0.4120\n",
      "Epoch 4/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8547 - loss: 0.4180\n",
      "Epoch 5/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9140 - loss: 0.1764\n",
      "Epoch 6/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9837 - loss: 0.0733\n",
      "Epoch 7/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9935 - loss: 0.0630\n",
      "Epoch 8/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9887 - loss: 0.0480\n",
      "Epoch 9/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9957 - loss: 0.0352\n",
      "Epoch 10/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9960 - loss: 0.0318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f244e5c2db0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(200, activation='relu', input_shape=(X.shape[1],)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "#compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#train the model\n",
    "model.fit(X, y, epochs=10, batch_size=32)\n",
    "\n",
    "features = X_df.columns.to_list()\n",
    "W=model.get_weights() # its an array of arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18875, 200, 200, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(W[0]), len(W[1]), len(W[2]), len(W[3])\n",
    "# W[0] array 18875x200 representing weights W of each feature in each of the 200 inner neurons\n",
    "# W[1] array of 200x1 represents biases b from each of the neuron\n",
    "# W[2] array od 200x1 representing the weights W of each of the z (neuron output in hidden layer) in the output neuron\n",
    "# W[3] array 1x1 representing the bias b used in teh last output neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 200\n",
    "\n",
    "\n",
    "models=[]\n",
    "for i in range(n_models):\n",
    "    X_boot, y_boot = resample(X, y, random_state=i)\n",
    "    model=SGDClassifier(loss='hinge', penalty= 'l1', max_iter=1000, tol=1e-3)\n",
    "    model.fit(X_boot, y_boot.ravel())\n",
    "    models.append(model)\n",
    "\n",
    "weights=np.zeros((n_models, X.shape[1]))\n",
    "for i in range(n_models):\n",
    "    weights[i]=models[i].coef_\n",
    "weights_df = pd.DataFrame(weights, columns=X_df.columns)\n",
    "\n",
    "corr_SVM = weights_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it in a csv\n",
    "corr_SVM.to_csv(f'../../data/temp/corr_SVM_{species}_{drug}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/temp/corr_SVM_Escherichia_coli_ciprofloxacin.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m corr_SVM \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../data/temp/corr_SVM_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mspecies\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdrug\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/temp/corr_SVM_Escherichia_coli_ciprofloxacin.csv'"
     ]
    }
   ],
   "source": [
    "corr_SVM = pd.read_csv(f'../../data/temp/corr_SVM_{species}_{drug}.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0.1 # -- UNSIGNED\n",
    "\n",
    "G=nx.Graph()\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "\n",
    "    for j in range(i+1, X.shape[1]):\n",
    "        node_i=X_df.columns[i]; node_j=X_df.columns[j]\n",
    "        correlation = corr_SVM.iloc[i, j]\n",
    "        if correlation > t:\n",
    "            G.add_edge(node_i, node_j, weight=correlation)\n",
    "\n",
    "# G=set_cluster_attributes(G)\n",
    "# nx.set_node_attributes(G, log_odds_dict, 'log_odds')\n",
    "\n",
    "nx.write_graphml(G, f'../../data/nets/{species}/{n_models}_randomized_SVM_{t}unsigned_corr_{drug}.graphml')\n",
    "\n",
    "edges_list = list(G.edges)\n",
    "edge_weights = [G[u][v]['weight'] for u, v in edges_list]\n",
    "print(f\"E: {len(edges_list)}, V: {G.number_of_nodes()} when {n_models} models are used, with corr threshold={t}; species={species}, drug={drug}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.write_graphml(G, f'../../data/nets/{species}/{n_models}_randomized_SVM_{t}unsigned_corr_{drug}.graphml')\n",
    "\n",
    "# edges_list = list(G.edges)\n",
    "# edge_weights = [G[u][v]['weight'] for u, v in edges_list]\n",
    "# print(f\"E: {len(edges_list)}, V: {G.number_of_nodes()} when {n_models} models are used, with corr threshold={t}; species={species}, drug={drug}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.read_graphml('../../data/nets/Escherichia_coli/200_randomized_SVM_0.2unsigned_corr_trimethoprim.graphml')\n",
    "edge_list = list(G.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "X_pairs = [(i.replace(' ', '_'), j.replace(' ', '_')) for i, j in edge_list]\n",
    "interaction_labeled_matrix = labeled_matrix.rename(columns={col: col.replace(' ', '_') for col in labeled_matrix.columns})\n",
    "\n",
    "# formula = f'SIR ~ {a} + {b} + {a}:{b}'\n",
    "\n",
    "test_pairs = X_pairs[:1]\n",
    "\n",
    "for pair in test_pairs:\n",
    "    a, b = pair\n",
    "    print(\" --- looking into the edge between\", a, \"and\", b)\n",
    "\n",
    "    results = smf.logit(formula=f'SIR ~ {a} + {b} + {a}:{b}', data=interaction_labeled_matrix).fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pairs=edge_list\n",
    "error_test=[(\"Cluster 0\", \"Cluster 20\")]\n",
    "\n",
    "p_values_list = []\n",
    "\n",
    "for pair in error_test:\n",
    "    a, b = pair\n",
    "    print(\" --- looking into the edge between\", a, \"and\", b)\n",
    "\n",
    "    X_selected = X_df.loc[:, [a, b]]\n",
    "\n",
    "    print(X_selected.shape)\n",
    "\n",
    "    X_selected = sm.add_constant(X_selected)\n",
    "\n",
    "    X_selected['interaction'] = X_selected.iloc[:, a] * X_selected.iloc[:, b]\n",
    "\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            logit_model = sm.Logit(y, X_selected)\n",
    "            result = logit_model.fit()\n",
    "            p_values_list.append(result.pvalues['interaction'])\n",
    "\n",
    "    except:\n",
    "        print('singular matrix; deleted')\n",
    "\n",
    "\n",
    "# reject, corrected_p_values= multipletests(p_values_list, method='fdr_bh')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = X_df.loc[:, [\"Cluster 0\", \"Cluster 20\"]]\n",
    "# -- check if the 2 cols are identical\n",
    "test = sm.add_constant(test)\n",
    "\n",
    "    # if X_selected.shape[1]==2:\n",
    "    #     X_selected['interaction'] = X_selected.iloc[:, 1] * X_selected.iloc[:, 0]\n",
    "    # X_selected['interaction'] = X_selected.iloc[:, 1] * X_selected.iloc[:, 2]\n",
    "\n",
    "test['interaction']=test.loc[:,\"Cluster 0\"] * test.loc[:,\"Cluster 20\"]\n",
    "test\n",
    "\n",
    "# X_selected\n",
    "# get corr\n",
    "# X_selected.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "np.random.seed(111)\n",
    "df = pd.DataFrame(np.random.binomial(1,0.5,(50,3)),columns=['x1','x2','y'])\n",
    "\n",
    "res1 = smf.logit(formula='y ~ x1 + x2 + x1:x2', data=df).fit()\n",
    "\n",
    "res1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smf.logit(formula='y ~ x1 + x2 + x1:x2', data=df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0; insig=0\n",
    "save_edges=[]\n",
    "p_values_list=[]\n",
    "\n",
    "for pair in edges_list:\n",
    "    a, b = pair\n",
    "    print(\" --- looking into the edge between\", a, \"and\", b)\n",
    "\n",
    "    X_selected = X_df.loc[:, [a, b]]\n",
    "\n",
    "    print(X_selected.shape)\n",
    "\n",
    "    X_selected = sm.add_constant(X_selected)\n",
    "    print(X_selected.shape)\n",
    "    if X_selected.shape[1]==2:\n",
    "        X_selected['interaction'] = X_selected.iloc[:, 1] * X_selected.iloc[:, 0]\n",
    "    X_selected['interaction'] = X_selected.iloc[:, 1] * X_selected.iloc[:, 2]\n",
    "\n",
    "    print(X_selected.shape)\n",
    "    \n",
    "    col=False\n",
    "\n",
    "\n",
    "    #account for collinearity\n",
    "    # --- matrix way\n",
    "    if np.linalg.matrix_rank(X_selected) < X_selected.shape[1]:\n",
    "        print('collinearity; deleted')\n",
    "        # G.remove_edge(*pair)\n",
    "        continue\n",
    "\n",
    "    # --- paper way\n",
    "    # if (X_selected['interaction']==X_selected[a]).all() or (X_selected['interaction']==X_selected[b]).all():      \n",
    "    #     col=True  \n",
    "    #     print(f'colinearity between interaction and one of the features {a} or {b}; deleted')\n",
    "    # elif (X_selected[b]==X_selected[a]).all():\n",
    "    #     col=True\n",
    "    #     print(f'colinearity between {a} and {b}; deleted')\n",
    "    # elif (X_selected['const']==X_selected[a]).all() or (X_selected['const']==X_selected[b]).all() or (X_selected['const']==X_selected['interaction']).all():\n",
    "    #     col=True\n",
    "    #     print('colinearity between one of the features and the constant term?!')\n",
    "\n",
    "    # if col:\n",
    "    #     G.remove_edge(*pair)\n",
    "    #     continue\n",
    "\n",
    "    # --- logistic regression\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            logit_model = sm.Logit(y, X_selected)\n",
    "            result = logit_model.fit()\n",
    "    except:\n",
    "        print('singular matrix; deleted')\n",
    "\n",
    "    p_values_list.append(p_values['interaction'])\n",
    "\n",
    "    # coefficients = result.params\n",
    "    p_values = result.pvalues\n",
    "\n",
    "    if p_values['interaction']<0.05:\n",
    "        print('not significant; deleted')\n",
    "        # G.remove_edge(*pair)\n",
    "        insig+=1\n",
    "    else:\n",
    "        count+=1\n",
    "        save_edges.append(pair)\n",
    "\n",
    "\n",
    "    # --- multiple testing Benjamini-Hochberg correction\n",
    "    reject, corrected_p_values= multipletests(p_values_list, method='fdr_bh')\n",
    "\n",
    "    for i, pair in enumerate(edges_list):\n",
    "        if corrected_p_values[i] < 0.05:\n",
    "            print(f'Edge {pair} is not significant; deleted')\n",
    "            G.remove_edge(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-way ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_2way_ANOVA(X_pairs, labeled_matrix):\n",
    "    '''\n",
    "    takes a list of tuples (pairs) - supposed to be pairs of features found in labeled_matrix - and a labeled matrix (X + y)\n",
    "    returns teh significant interaction (those having corr p_val<0.05) \n",
    "    accounting for multiple testing, using the Benjamini-Hochberg FDR correction\n",
    "\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pairs = [(i.replace(' ', '_'), j.replace(' ', '_')) for i, j in edge_list]\n",
    "interaction_labeled_matrix = labeled_matrix.rename(columns={col: col.replace(' ', '_') for col in labeled_matrix.columns})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_list = list(G.edges)\n",
    "edge_weights = [G[u][v]['weight'] for u, v in edges_list]\n",
    "print(f\"E: {len(edges_list)}, V: {G.number_of_nodes()} when {n_models} models are used, with corr threshold={t}; species={species}, drug={drug}.\")\n",
    "\n",
    "save_edges=[]\n",
    "\n",
    "edges_list_2=[(i.replace(' ', '_'), j.replace(' ', '_')) for i, j in edges_list]\n",
    "labeled_matrix_2 = labeled_matrix.rename(columns={col: col.replace(' ', '_') for col in labeled_matrix.columns})\n",
    "p_values={}\n",
    "\n",
    "count_nan=0\n",
    "save_nan_testing=[]\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for pair in edges_list_2:\n",
    "        a, b = pair\n",
    "        # print(\" --- looking into the interaction between\", a, \"and\", b)\n",
    "        formula = f'SIR ~ {a} + {b} + {a}:{b}'\n",
    "        model = ols(formula, data=labeled_matrix_2).fit()\n",
    "        anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "        if not np.isnan(anova_table['PR(>F)'][2]):\n",
    "            p_values[anova_table.index[2]]=anova_table['PR(>F)'][2]\n",
    "        else:\n",
    "            save_nan_testing.append(pair)\n",
    "            count_nan+=1\n",
    "        \n",
    "print(f\"{count_nan} interactions have nan p-values - removed\")\n",
    "\n",
    "# mst correction - using fdr\n",
    "\n",
    "gene_gene_pair = list(p_values.keys())\n",
    "p_values_list = list(p_values.values())\n",
    "\n",
    "reject, corrected_p_values= fdrcorrection(p_values_list)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'gene_gene': gene_gene_pair,\n",
    "    'p-value': p_values_list,\n",
    "    'corr_p_value': corrected_p_values,\n",
    "    'reject_H0': reject\n",
    "})\n",
    "\n",
    "count_rejected=0\n",
    "for i in reject:\n",
    "    if i:\n",
    "        count_rejected+=1\n",
    "\n",
    "print(f\"Number of rejected H0: {count_rejected} out of {len(reject)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOR of co-occurence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clstr_df= get_cluster_representatives(f'../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}.fasta.clstr')\n",
    "_product_df=get_representative_products(f'../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}.fasta')\n",
    "product_df = combine_cluster_product(clstr_df, _product_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  left joing X_df and product_df on the index\n",
    "X_t= X_df.T\n",
    "test = X_t.join(product_df, how='left')\n",
    "l=test['product_name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Cluster'] = test.index\n",
    "test.index = test['product_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_product_df.index= _product_df['product_name']\n",
    "p_ =_product_df.drop('product_name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner join test and _product_df on the index\n",
    "test = test.join(_product_df, how='inner', lsuffix='_caller', rsuffix='_other')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['product_name'].to_list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
