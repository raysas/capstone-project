{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 11:52:41.359168: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-04 11:52:41.360633: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-04 11:52:41.435004: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-04 11:52:41.722454: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-04 11:52:42.525992: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "from cluster_analysis import *\n",
    "# from LOR_calculation import *\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mtb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species='Mycobacterium_tuberculosis'\n",
    "\n",
    "genome_ids_file=f\"../../pangenome-repo/Pangenome-Analysis-Workflow/genome_ids/{species}_genome_ids.csv\"\n",
    "clstr_freq_file=f\"../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}_cluster_frequencies.csv\"\n",
    "clstr_file=f\"../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}.fasta.clstr\"\n",
    "clstr_fasta_file=f\"../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}.fasta\"\n",
    "\n",
    "samples_df = pd.read_csv(genome_ids_file, dtype=str)\n",
    "samples_list=samples_df['genome.genome_ids'].tolist()\n",
    "\n",
    "clstr_freq_df=pd.read_csv(clstr_freq_file, index_col=1)\n",
    "clstr_freq_df=clstr_freq_df.drop(clstr_freq_df.columns[0], axis=1)\n",
    "clstr_list=clstr_freq_df.index.tolist()\n",
    "\n",
    "df = pd.DataFrame(index=samples_list, columns=clstr_list)\n",
    "df = df.fillna(0)\n",
    "\n",
    "with open(clstr_file) as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\">Cluster\"):\n",
    "            #if its a > line get the cluster id and start counting its occurences in which samples by saving it to the local var\n",
    "            cluster_id=line[1:].strip()\n",
    "        else:\n",
    "            #else its a line designating the sample genome, one that the last cluster_id is present in\n",
    "            # get the 1st group matching here \\d\\t\\d+aa, >fig\\|([\\d\\.\\d]+).+\n",
    "\n",
    "            genome_id=re.match(r\"\\d+\\t\\d+aa, >fig\\|([\\d\\.\\d]+).+\", line).group(1)\n",
    "            genome_id=genome_id[:-1]  #removing the trailing .\n",
    "            df.loc[genome_id, cluster_id]+=1\n",
    "\n",
    "df = df.T\n",
    "df.to_csv(f'../../data/presence_matrices/{species}_GxS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species='Mycobacterium_tuberculosis'; drug=''\n",
    "\n",
    "presence_df = pd.read_csv(f'../../data/presence_matrices/{species}_GxS.csv', index_col=0); \n",
    "X_df=presence_df.T\n",
    "X_df.index = X_df.index.astype('float')\n",
    "# X_df.index = X_df.index.astype('object')\n",
    "\n",
    "\n",
    "pheno_df= pd.read_csv(f'../../data/phenotypes/{species}_{drug}.csv', index_col=0)\n",
    "y_df=pheno_df\n",
    "y_df.index = y_df.index.astype('float')\n",
    "\n",
    "for gene in X_df.columns:\n",
    "    \n",
    "    if X_df[gene].std() == 0:\n",
    "        X_df.drop(gene, axis=1, inplace=True)\n",
    "\n",
    "X_df = X_df.sort_index()\n",
    "y_df = y_df.sort_index()\n",
    "\n",
    "\n",
    "y_indices=list(y_df.index)\n",
    "X_indices=list(X_df.index)\n",
    "\n",
    "intersection = [i for i in y_indices if i in X_indices]\n",
    "y_df = y_df.loc[intersection]\n",
    "X_df = X_df.loc[intersection]\n",
    "\n",
    "# -- removing duplicate rows? --          to check th e nature of duplicates it could be an error through type conversion\n",
    "X_df = X_df.loc[~X_df.index.duplicated(keep='first')]\n",
    "\n",
    "X = X_df.values\n",
    "y = y_df.values\n",
    "\n",
    "labeled_matrix = pd.concat([X_df, y_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E coli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "species='Escherichia_coli'\n",
    "\n",
    "X_df = pd.read_csv(f'../../data/temp/X_{species}_SxG_filtered_hypo_unknowns_freq5.csv', index_col=0) # filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1970, 18875)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# species='Escherichia_coli'\n",
    "\n",
    "# genome_ids_file=f\"../../pangenome-repo/Pangenome-Analysis-Workflow/genome_ids/{species}_genome_ids.csv\"\n",
    "# clstr_freq_file=f\"../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}_cluster_frequencies.csv\"\n",
    "# clstr_file=f\"../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}.fasta.clstr\"\n",
    "# clstr_fasta_file=f\"../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}.fasta\"\n",
    "\n",
    "# samples_df = pd.read_csv(genome_ids_file, dtype=str)\n",
    "# samples_list=samples_df['genome.genome_ids'].tolist()\n",
    "\n",
    "# clstr_freq_df=pd.read_csv(clstr_freq_file, index_col=1)\n",
    "# clstr_freq_df=clstr_freq_df.drop(clstr_freq_df.columns[0], axis=1)\n",
    "# clstr_list=clstr_freq_df.index.tolist()\n",
    "\n",
    "# df = pd.DataFrame(index=samples_list, columns=clstr_list)\n",
    "# df = df.fillna(0)\n",
    "\n",
    "# with open(clstr_file) as f:\n",
    "#     for line in f:\n",
    "#         if line.startswith(\">Cluster\"):\n",
    "#             #if its a > line get the cluster id and start counting its occurences in which samples by saving it to the local var\n",
    "#             cluster_id=line[1:].strip()\n",
    "#         else:\n",
    "#             #else its a line designating the sample genome, one that the last cluster_id is present in\n",
    "#             # get the 1st group matching here \\d\\t\\d+aa, >fig\\|([\\d\\.\\d]+).+\n",
    "\n",
    "#             genome_id=re.match(r\"\\d+\\t\\d+aa, >fig\\|([\\d\\.\\d]+).+\", line).group(1)\n",
    "#             genome_id=genome_id[:-1]  #removing the trailing .\n",
    "#             df.loc[genome_id, cluster_id]+=1\n",
    "\n",
    "# df = df.T\n",
    "# df.to_csv(f'../../data/presence_matrices/{species}_GxS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filtration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unknowns and hypotheticals and freq < 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clstr_df= get_cluster_representatives(f'../../data/pangenome_pipeline_output/{species}/{species}.fasta.clstr')\n",
    "_product_df=get_representative_products(f'../../data/pangenome_pipeline_output/{species}/{species}.fasta')\n",
    "product_df = combine_cluster_product(clstr_df, _product_df)\n",
    "freq_df = get_cluster_frequency(f'../../data/pangenome_pipeline_output/{species}/{species}_cluster_frequencies.csv')\n",
    "\n",
    "#  --counting those thar have hypotehtical in the product name\n",
    "product_df['hypothetical'] = product_df['product_name'].str.contains('hypothetical', case=False)\n",
    "product_df['hypothetical'].sum()\n",
    "\n",
    "#  --same for unknown\n",
    "product_df['unknown'] = product_df['product_name'].str.contains('unknown', case=False)\n",
    "product_df['unknown'].sum()\n",
    "\n",
    "# now will get a list of clusters that are either hypo or unknown to filter out\n",
    "hypothetical_clusters = product_df[product_df['hypothetical']==True].index.tolist()\n",
    "unknown_clusters = product_df[product_df['unknown']==True].index.tolist()\n",
    "\n",
    "to_remove = list(set(hypothetical_clusters).union(unknown_clusters))\n",
    "\n",
    "# -- get the list of clusters that are present in less than 5 genomes\n",
    "low_freq_clusters = freq_df[freq_df['frequency']<5].index.tolist()\n",
    "\n",
    "to_remove = list(set(to_remove).union(low_freq_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "presence_df = pd.read_csv(f'../../data/presence_matrices/{species}_GxS.csv', index_col=0); \n",
    "X_df=presence_df.T\n",
    "X_df.index = X_df.index.astype('float')\n",
    "# X_df.index = X_df.index.astype('object')\n",
    "\n",
    "# -- removing hypothetical and unknown clusters nad those present in lesss than 5 samples\n",
    "X_df = X_df.drop(to_remove, axis=1)\n",
    "\n",
    "# -- memoized for later use if needed\n",
    "# X_df.to_csv(f'../../data/presence_matrices/{species}_filtered_SxG.csv.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.to_csv(f'../../data/presence_matrices/{species}_filtered_SxG.csv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  -- removing genes belowe a variance threshold\n",
    "v_t=0.1\n",
    "\n",
    "for gene in X_df.columns:\n",
    "    if X_df[gene].var() < v_t: \n",
    "        X_df.drop(gene, axis=1, inplace=True)\n",
    "\n",
    "X_df.to_csv(f'../../data/temp/X_{species}_SxG_filtered_var_{v_t}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species='Escherichia_coli'; drug=''; v_t=0.1\n",
    "\n",
    "X_df = pd.read_csv(f'../../data/temp/X_{species}_SxG_filtered_var_{v_t}.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## labels - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimethoprim = ['folA','dfrA','dfrG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs = ['streptomycin',\n",
    " 'sulfamethoxazole',\n",
    " 'tetracycline',\n",
    " 'cefalothin',\n",
    " 'trimethoprim_sulphamethoxazole',\n",
    " 'amoxicillin_clavulanic_acid',\n",
    " 'trimethoprim',\n",
    " 'amoxicillin',\n",
    " 'ampicillin',\n",
    " 'doripenem',\n",
    " 'levofloxacin',\n",
    " 'ciprofloxacin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499, 18876)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug='trimethoprim'\n",
    "\n",
    "pheno_df= pd.read_csv(f'../../metadata/{species}/{species}_{drug}.csv', index_col=0)\n",
    "y_df=pheno_df\n",
    "y_df.index = y_df.index.astype('float')\n",
    "\n",
    "y_df = y_df.sort_index()\n",
    "\n",
    "y_indices=list(y_df.index)\n",
    "\n",
    "\n",
    "X_df = X_df.sort_index()\n",
    "y_df = y_df.sort_index()\n",
    "\n",
    "y_indices=list(y_df.index)\n",
    "X_indices=list(X_df.index)\n",
    "\n",
    "intersection = [i for i in y_indices if i in X_indices]\n",
    "y_df = y_df.loc[intersection]\n",
    "X_df = X_df.loc[intersection]\n",
    "\n",
    "X_df = X_df.sort_index()\n",
    "y_df = y_df.sort_index() # -- just making sure bcs im paranoid\n",
    "\n",
    "X = X_df.values\n",
    "y = y_df.values\n",
    "\n",
    "\n",
    "\n",
    "labeled_matrix = pd.concat([X_df, y_df], axis=1)\n",
    "labeled_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dihydrofolate reductase (EC 1.5.1.3) for folA\n",
    "\n",
    "product_df = product_df.drop(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  -- dont forget to add this to a module (maybe cluster analysis)\n",
    "def transform_cluster_to_product(cluster_name, clstr_product_df):\n",
    "    #match it ffrom clstr_poduct_df from the index\n",
    "    cluster = clstr_product_df.loc[cluster_name]\n",
    "    product=cluster['product_name']\n",
    "    \n",
    "    return product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to perform different statistical and machine learning approaches to associate each feature (gene) with the phenotype of interest (one drug at a time).  _The phenotype of interest is the resistance to a specific drug._  \n",
    "Would lated take the top 100 genes of each approach and compare how 1. if the known ARGs are ranked high in each and 2. if there is enough overlap. Would probably combine all to create a pool of genes to test for interaction later on.\n",
    "\n",
    "Ideas on statistical approaches:  \n",
    "- Mutual Information (MI)\n",
    "- Fisher's exact test/Chi-squared test\n",
    "- ANOVA\n",
    "- Jaccard index\n",
    "\n",
    "Ideas on machine learning approaches:\n",
    "- Random Forest\n",
    "- SVM\n",
    "- Logistic Regression\n",
    "- Gradient Boosting\n",
    "- Neural Networks\n",
    "\n",
    "Would probably start with ensemble methods, using weights sum/mean as the final score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ARGs(ARG_products:pd.DataFrame, top_genes:pd.DataFrame):\n",
    "    '''\n",
    "    Takes in a list of ARG products and a list of top genes and returns a dict of ARGs that are in the top genes (with their rank)\n",
    "    '''\n",
    "    ARGs = {}\n",
    "    for gene in top_genes:\n",
    "        for product in ARG_products:\n",
    "            if product in gene:\n",
    "                ARGs[top_genes.index(gene)]=gene\n",
    "    return ARGs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- computing MI between each col and the last col in labeled_matrix\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "mi_scores = {}\n",
    "for col in labeled_matrix.columns[:-1]:\n",
    "    mi_scores[col]=mutual_info_score(labeled_matrix[col], labeled_matrix['SIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- computing MI between each col and the last col in labeled_matrix\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "mi_scores = {}\n",
    "for col in labeled_matrix.columns[:-1]:\n",
    "    mi_scores[col]=mutual_info_score(labeled_matrix[col], labeled_matrix['SIR'])\n",
    "\n",
    "MI_top_100 = sorted(mi_scores, key=mi_scores.get, reverse=True)[:100]\n",
    "for i in  range(len(MI_top_100)):\n",
    "    MI_top_100[i]=transform_cluster_to_product(MI_top_100[i], product_df)\n",
    "\n",
    "parse_ARGs(trimethoprim_products, MI_top_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting top 100 scores\n",
    "\n",
    "MI_top_100 = sorted(mi_scores, key=mi_scores.get, reverse=True)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in  range(len(MI_top_100)):\n",
    "    MI_top_100[i]=transform_cluster_to_product(MI_top_100[i], product_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dihydropteroate synthase type-2 (EC 2.5.1.15) @ Sulfonamide resistance protein_7',\n",
       " 'Integron integrase IntI1_2',\n",
       " 'Aminoglycoside 6-phosphotransferase (EC 2.7.1.72) => APH(6)-Ic/APH(6)-Id_11',\n",
       " 'Small multidrug resistance (SMR) efflux transporter => QacE delta 1, quaternary ammonium compounds_4',\n",
       " 'Dihydropteroate synthase type-2 (EC 2.5.1.15) @ Sulfonamide resistance protein_4',\n",
       " 'Mercuric transport protein, MerC_1',\n",
       " \"Aminoglycoside 3''-nucleotidyltransferase (EC 2.7.7.-) => ANT(3'')-Ia (AadA family)_6\",\n",
       " 'Chromate transport protein ChrA_3',\n",
       " \"Aminoglycoside 3''-phosphotransferase (EC 2.7.1.87) => APH(3'')-I_1\",\n",
       " 'Mercuric transport protein, MerT_2',\n",
       " 'Periplasmic mercury(+2) binding protein, MerP_1',\n",
       " 'Mercuric ion reductase (EC 1.16.1.1)_1',\n",
       " 'Mercuric resistence transcriptional repressor, MerD_1',\n",
       " 'Mercuric resistance operon regulatory protein MerR_1',\n",
       " 'T6SS component TssM (IcmF/VasK)_18',\n",
       " \"Macrolide 2'-phosphotransferase => Mph(A) family_1\",\n",
       " 'Uncharacterized GST-like protein yncG_4',\n",
       " 'Mobile element protein_121',\n",
       " 'Glutamate decarboxylase (EC 4.1.1.15)_1',\n",
       " 'Mercuric transport protein, MerE_1',\n",
       " 'Dihydrofolate reductase (EC 1.5.1.3)_1',\n",
       " 'Uncharacterized fimbrial-like protein YadL_2',\n",
       " 'H repeat-associated protein, YhhI family_65',\n",
       " 'Class A beta-lactamase (EC 3.5.2.6) => TEM family_1',\n",
       " 'Outer membrane porin OmpD_3',\n",
       " '3-(3-hydroxyphenyl)propanoate hydroxylase (EC 1.14.13.127)_1',\n",
       " 'Tetracycline resistance, MFS efflux pump => Tet(A)_6',\n",
       " 'Tetracycline resistance regulatory protein TetR_1',\n",
       " 'Uncharacterized fimbrial-like protein YadM_16',\n",
       " 'IS, phage, Tn; Transposon-related functions_8',\n",
       " 'Diguanylate cyclase (EC 2.7.7.65) => Globin-coupled heme-based oxygen sensor DgcO_4',\n",
       " 'Phage tail fiber, side tail fiber protein Stf_458',\n",
       " 'Uncharacterized fimbrial-like protein YadK_4',\n",
       " 'Per-activated serine protease autotransporter enterotoxin EspC_11',\n",
       " '3-(3-hydroxyphenyl)propionate transporter, MFS-type_1',\n",
       " 'Mobile element protein_103',\n",
       " 'Replication protein RepC_2',\n",
       " '4-hydroxy-2-oxovalerate aldolase (EC 4.1.3.39)_2',\n",
       " 'Acetaldehyde dehydrogenase, acetylating, (EC 1.2.1.10) in gene cluster for degradation of phenols, cresols, catechol_1',\n",
       " '2-hydroxy-6-oxonona-2,4-dienedioate hydrolase (EC 3.7.1.14)_1',\n",
       " '2-keto-4-pentenoate hydratase (EC 4.2.1.80)_1',\n",
       " 'Replication protein RepA_7',\n",
       " '3-carboxyethylcatechol 2,3-dioxygenase (EC 1.13.11.16)_2',\n",
       " 'O-antigen ligase_2',\n",
       " 'Beta-1,3-glucosyltransferase_1',\n",
       " 'Integrase_54',\n",
       " 'Minor fimbrial subunit StfE_6',\n",
       " 'TPR domain protein_8',\n",
       " 'Transposase_203',\n",
       " 'DNA replication protein_44',\n",
       " 'Mhp operon transcriptional activator_2',\n",
       " 'Carbamate kinase (EC 2.7.2.2)_2',\n",
       " 'Arginine pathway regulatory protein ArgR, repressor of arg regulon_3',\n",
       " 'RidA/YER057c/UK114 superfamily, group 7, YjgH-like protein_1',\n",
       " 'Putative HTH-type transcriptional regulator YjgJ, TetR family_2',\n",
       " 'Putative uncharacterized protein YhiS_4',\n",
       " 'Cyclic-di-GMP-binding biofilm dispersal mediator protein_1',\n",
       " 'Fructose-1,6-bisphosphatase, GlpX type (EC 3.1.3.11)_9',\n",
       " 'Mobile element protein_232',\n",
       " 'Arginine/ornithine antiporter ArcD_5',\n",
       " 'Glycosyltransferase_18',\n",
       " 'Mobile element protein_456',\n",
       " 'Minor fimbrial subunit StfF_7',\n",
       " 'TniA putative transposase_11',\n",
       " 'PilV-like protein_7',\n",
       " 'Mobile element protein_24',\n",
       " 'Fructose-1,6-bisphosphatase, GlpX type (EC 3.1.3.11)_8',\n",
       " 'AidA-I adhesin-like protein_79',\n",
       " 'Nucleoside-specific channel-forming protein Tsx precursor_7',\n",
       " 'Inner membrane protein YjgN_25',\n",
       " 'Uncharacterized protein YbeU_4',\n",
       " 'Uncharacterized MFS-type transporter_58',\n",
       " 'Polyketide synthase modules and related proteins_6',\n",
       " 'Polyketide synthase modules and related proteins_2',\n",
       " 'Polyketide synthase modules and related proteins_4',\n",
       " 'Polyketide synthase modules and related proteins_8',\n",
       " 'Polyketide synthase modules and related proteins_5',\n",
       " 'Putative amidase_3',\n",
       " 'Na+-driven multidrug efflux pump_1',\n",
       " 'Integrase_21',\n",
       " 'Acyl-CoA dehydrogenase_2',\n",
       " 'Uncharacterized protein YbeQ_2',\n",
       " 'Transposase, IS3/IS911 family_2',\n",
       " \"5'-nucleotidase (EC 3.1.3.5)_5\",\n",
       " 'Chaperone protein fimC precursor_8',\n",
       " 'Phage exclusion protein ren_2',\n",
       " 'Polyketide synthase modules and related proteins_7',\n",
       " 'Polyketide synthase modules and related proteins_3',\n",
       " 'Arylsulfatase (EC 3.1.6.1)_17',\n",
       " 'Methyl-accepting chemotaxis protein III (ribose and galactose chemoreceptor protein)_6',\n",
       " 'Uncharacterized Na(+)/H(+) exchanger YjcE_5',\n",
       " 'Malonyl CoA-acyl carrier protein transacylase (EC 2.3.1.39) in polyketide synthesis_1',\n",
       " 'Programmed cell death toxin PemK_1',\n",
       " 'AidA-I adhesin-like protein_11',\n",
       " 'T6SS outer membrane component TssL (ImpK/VasF)_1',\n",
       " 'T6SS secretion lipoprotein TssJ (VasD)_1',\n",
       " 'Methyl-accepting chemotaxis protein IV (dipeptide chemoreceptor protein)_1',\n",
       " 'Methyl-accepting chemotaxis protein IV (dipeptide chemoreceptor protein)_2',\n",
       " 'RatA homolog_2',\n",
       " 'Programmed cell death antitoxin PemI_1']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MI_top_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{20: 'Dihydrofolate reductase (EC 1.5.1.3)_1'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- checking if tetramethoprim are there\n",
    "trimethoprim = ['folA','dfrA','dfrG']\n",
    "trimethoprim_products = ['Dihydrofolate reductase (EC 1.5.1.3)','trimethoprim-resistant dihydrofolate reductase DfrA1','Dihydrofolate reductase (EC 1.5.1.3)']\n",
    "\n",
    "parse_ARGs(trimethoprim_products, MI_top_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to detect interactions between the pool of genes we now have, also following statictical and machine learning approaches.\n",
    "\n",
    "Statistical approaches:\n",
    "- 2 way ANOVA\n",
    "- correlation (rho) t test (this should be coupled with a ML ensembl)\n",
    "\n",
    "Machine learning approaches:\n",
    "- Logistic Regression with interaction terms\n",
    "- SVM ensemble\n",
    "Following the assumption in Kavvas et al. (2018), we can use SVM and consider high correlation between features as a sign of interaction.\n",
    "- Neural Networks\n",
    "They mentioned that it would be nice to consider other models in other works. I think NN would be a good candidate because, even though it non-interpretable, it can capture complex relationship between features, if we can find a way to correlate the weights with the features. For this purpose, we might consider \n",
    "\n",
    "_For computational complexity, considering the interactions to test for significance our of the SVMs correlation_\n",
    "\n",
    "\n",
    "**ALWAYS ACCOUNTING FOR MULTIPLE TESTING USING THE ENJAMINI-HOCHBERG CORRECTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-01 20:26:49.376205: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-01 20:26:49.377380: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-01 20:26:49.407931: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-01 20:26:49.537407: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-01 20:26:50.283588: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ANN_cor(X_df, y_df):\n",
    "    '''\n",
    "    takes X_df (SxG) and y_df (SIRs, Sx1) and returns the correlation matrix of the weights of the ANN\n",
    "\n",
    "    The idea behind it:\n",
    "        similar to teh assumption that 2 features on the same hyperplane of an SVM might interact,\n",
    "        2 features with correalted weights through all the inner neurons of the hidden layers might interact\n",
    "\n",
    "    The neural network n=trained is very simplitic: \n",
    "        - depth=1\n",
    "        - number of neurons=200\n",
    "        - epochs=10\n",
    "        - activation functions: rely, sigmoid respectively\n",
    "        - loss function: binary crossentropy\n",
    "        - optimizer fucntion: Adam\n",
    "\n",
    "    The data is not split to train and test because the goal is not to predict but to get interactions, and we need as much data as can be found\n",
    "\n",
    "    param:\n",
    "    -------\n",
    "        - X_df: pd.DataFrame\n",
    "        - y_df: pd.Dataframe\n",
    "    \n",
    "    return:\n",
    "    -------\n",
    "        - corr_matrix: pd.DataFrame\n",
    "    '''\n",
    "    X = X_df.values\n",
    "    y = y_df.values\n",
    "\n",
    "    features = X_df.columns.to_list()\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(200, activation='relu', input_shape=(X.shape[1],)),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X, y, epochs=10, batch_size=32)\n",
    "\n",
    "    W = model.get_weights()\n",
    "\n",
    "    W_df = pd.DataFrame(W[0])\n",
    "    W_df = W_df.T\n",
    "    W_df.columns = features\n",
    "    corr_matrix = W_df.corr()\n",
    "\n",
    "    return corr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rayane/miniconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5548 - loss: 2.6732\n",
      "Epoch 2/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6400 - loss: 0.7963\n",
      "Epoch 3/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7656 - loss: 0.4806\n",
      "Epoch 4/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8432 - loss: 0.4052\n",
      "Epoch 5/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8697 - loss: 0.3549\n",
      "Epoch 6/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8973 - loss: 0.3086\n",
      "Epoch 7/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9079 - loss: 0.2833\n",
      "Epoch 8/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9318 - loss: 0.2367\n",
      "Epoch 9/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9203 - loss: 0.2315\n",
      "Epoch 10/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9616 - loss: 0.1844\n"
     ]
    }
   ],
   "source": [
    "corr_ANN = get_ANN_cor(X_df, y_df)\n",
    "corr_ANN.to_csv(f'../../data/temp/corr_ANN_{species}_{drug}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "corr_ANN = pd.read_csv(f'../../data/temp/corr_ANN_{species}_{drug}.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0.2 # -- Signed\n",
    "\n",
    "G=nx.Graph()\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "\n",
    "    for j in range(i+1, X.shape[1]):\n",
    "        node_i=X_df.columns[i]; node_j=X_df.columns[j]\n",
    "        correlation = corr_ANN.iloc[i, j]\n",
    "        if correlation > t or correlation < -t:\n",
    "            G.add_edge(node_i, node_j, weight=correlation)\n",
    "\n",
    "# G=set_cluster_attributes(G)\n",
    "# nx.set_node_attributes(G, log_odds_dict, 'log_odds')\n",
    "\n",
    "nx.write_graphml(G, f'../../data/nets/{species}/ANN_{t}signed_corr_{drug}.graphml')\n",
    "\n",
    "edges_list = list(G.edges)\n",
    "edge_weights = [G[u][v]['weight'] for u, v in edges_list]\n",
    "print(f\"E: {len(edges_list)}, V: {G.number_of_nodes()} when ANN are used, with corr threshold={t} (signed); species={species}, drug={drug}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rayane/miniconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5658 - loss: 2.2558\n",
      "Epoch 2/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7014 - loss: 1.1917\n",
      "Epoch 3/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8501 - loss: 0.4154\n",
      "Epoch 4/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9445 - loss: 0.1453\n",
      "Epoch 5/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9703 - loss: 0.1144\n",
      "Epoch 6/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9538 - loss: 0.1267\n",
      "Epoch 7/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9838 - loss: 0.0569\n",
      "Epoch 8/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9951 - loss: 0.0347\n",
      "Epoch 9/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9987 - loss: 0.0414\n",
      "Epoch 10/10\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9985 - loss: 0.0282\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(200, activation='relu', input_shape=(X.shape[1],)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y, epochs=10, batch_size=32)\n",
    "\n",
    "features = X_df.columns.to_list()\n",
    "W=model.get_weights() # its an array of arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18875, 200, 200, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(W[0]), len(W[1]), len(W[2]), len(W[3])\n",
    "# W[0] array 18875x200 representing weights W of each feature in each of the 200 inner neurons\n",
    "# W[1] array of 200x1 represents biases b from each of the neuron\n",
    "# W[2] array od 200x1 representing the weights W of each of the z (neuron output in hidden layer) in the output neuron\n",
    "# W[3] array 1x1 representing the bias b used in teh last output neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01123712, -0.00350125, -0.00385186, ..., -0.01668218,\n",
       "        -0.01172255,  0.00077817],\n",
       "       [-0.00777536, -0.01279151,  0.00665518, ...,  0.00041759,\n",
       "         0.00588902, -0.00848689],\n",
       "       [-0.003946  ,  0.0089682 ,  0.01430061, ...,  0.01249866,\n",
       "         0.014623  , -0.0171106 ],\n",
       "       ...,\n",
       "       [ 0.00032388, -0.01398619,  0.00596437, ...,  0.01543885,\n",
       "         0.00313141,  0.01487323],\n",
       "       [-0.01723274, -0.00217008,  0.01060628, ...,  0.00269783,\n",
       "         0.00570758,  0.00364065],\n",
       "       [ 0.01495725, -0.01098188,  0.00967708, ...,  0.00678905,\n",
       "        -0.00858929, -0.00240753]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster 0</th>\n",
       "      <th>Cluster 2</th>\n",
       "      <th>Cluster 4</th>\n",
       "      <th>Cluster 6</th>\n",
       "      <th>Cluster 9</th>\n",
       "      <th>Cluster 10</th>\n",
       "      <th>Cluster 15</th>\n",
       "      <th>Cluster 17</th>\n",
       "      <th>Cluster 18</th>\n",
       "      <th>Cluster 19</th>\n",
       "      <th>...</th>\n",
       "      <th>Cluster 90902</th>\n",
       "      <th>Cluster 90903</th>\n",
       "      <th>Cluster 90906</th>\n",
       "      <th>Cluster 90925</th>\n",
       "      <th>Cluster 90929</th>\n",
       "      <th>Cluster 90932</th>\n",
       "      <th>Cluster 90934</th>\n",
       "      <th>Cluster 90936</th>\n",
       "      <th>Cluster 90937</th>\n",
       "      <th>Cluster 90938</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.011237</td>\n",
       "      <td>-0.007775</td>\n",
       "      <td>-0.003946</td>\n",
       "      <td>0.011932</td>\n",
       "      <td>-0.013293</td>\n",
       "      <td>0.004583</td>\n",
       "      <td>-0.013791</td>\n",
       "      <td>0.007427</td>\n",
       "      <td>-0.013434</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007301</td>\n",
       "      <td>0.001219</td>\n",
       "      <td>0.004761</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>-0.011789</td>\n",
       "      <td>0.011120</td>\n",
       "      <td>0.015102</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>-0.017233</td>\n",
       "      <td>0.014957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.003501</td>\n",
       "      <td>-0.012792</td>\n",
       "      <td>0.008968</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.006941</td>\n",
       "      <td>-0.001278</td>\n",
       "      <td>0.014176</td>\n",
       "      <td>0.026523</td>\n",
       "      <td>-0.008823</td>\n",
       "      <td>0.009218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011292</td>\n",
       "      <td>0.006556</td>\n",
       "      <td>-0.010139</td>\n",
       "      <td>-0.008745</td>\n",
       "      <td>-0.008215</td>\n",
       "      <td>-0.006803</td>\n",
       "      <td>-0.014237</td>\n",
       "      <td>-0.013986</td>\n",
       "      <td>-0.002170</td>\n",
       "      <td>-0.010982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.003852</td>\n",
       "      <td>0.006655</td>\n",
       "      <td>0.014301</td>\n",
       "      <td>-0.005365</td>\n",
       "      <td>0.012869</td>\n",
       "      <td>0.009780</td>\n",
       "      <td>-0.012053</td>\n",
       "      <td>-0.006668</td>\n",
       "      <td>0.005272</td>\n",
       "      <td>-0.008114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003413</td>\n",
       "      <td>-0.012003</td>\n",
       "      <td>-0.012839</td>\n",
       "      <td>0.015831</td>\n",
       "      <td>0.015738</td>\n",
       "      <td>-0.008493</td>\n",
       "      <td>0.007017</td>\n",
       "      <td>0.005964</td>\n",
       "      <td>0.010606</td>\n",
       "      <td>0.009677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.005686</td>\n",
       "      <td>0.016590</td>\n",
       "      <td>-0.002658</td>\n",
       "      <td>0.004458</td>\n",
       "      <td>0.014662</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>0.012888</td>\n",
       "      <td>0.026360</td>\n",
       "      <td>-0.008801</td>\n",
       "      <td>-0.011197</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015570</td>\n",
       "      <td>-0.010009</td>\n",
       "      <td>0.011598</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>-0.008722</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>-0.014814</td>\n",
       "      <td>-0.010106</td>\n",
       "      <td>0.015868</td>\n",
       "      <td>-0.007455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015104</td>\n",
       "      <td>0.015619</td>\n",
       "      <td>0.004610</td>\n",
       "      <td>0.010777</td>\n",
       "      <td>-0.000738</td>\n",
       "      <td>-0.008183</td>\n",
       "      <td>0.012814</td>\n",
       "      <td>0.016787</td>\n",
       "      <td>-0.007946</td>\n",
       "      <td>0.004569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017268</td>\n",
       "      <td>-0.007491</td>\n",
       "      <td>-0.009670</td>\n",
       "      <td>0.007494</td>\n",
       "      <td>-0.004423</td>\n",
       "      <td>-0.005728</td>\n",
       "      <td>-0.000304</td>\n",
       "      <td>-0.015757</td>\n",
       "      <td>-0.008761</td>\n",
       "      <td>0.011665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.013102</td>\n",
       "      <td>-0.015427</td>\n",
       "      <td>-0.014379</td>\n",
       "      <td>0.004253</td>\n",
       "      <td>0.002515</td>\n",
       "      <td>-0.016599</td>\n",
       "      <td>-0.014148</td>\n",
       "      <td>-0.009423</td>\n",
       "      <td>-0.016268</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009657</td>\n",
       "      <td>0.017060</td>\n",
       "      <td>0.013498</td>\n",
       "      <td>-0.006362</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>-0.008498</td>\n",
       "      <td>0.012576</td>\n",
       "      <td>-0.017683</td>\n",
       "      <td>0.011611</td>\n",
       "      <td>-0.004294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>-0.004005</td>\n",
       "      <td>-0.000765</td>\n",
       "      <td>0.015550</td>\n",
       "      <td>0.016883</td>\n",
       "      <td>-0.014563</td>\n",
       "      <td>0.008051</td>\n",
       "      <td>0.012932</td>\n",
       "      <td>-0.012264</td>\n",
       "      <td>-0.013333</td>\n",
       "      <td>-0.005651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>-0.010362</td>\n",
       "      <td>0.001586</td>\n",
       "      <td>-0.017182</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>-0.014994</td>\n",
       "      <td>-0.002948</td>\n",
       "      <td>-0.005707</td>\n",
       "      <td>-0.009020</td>\n",
       "      <td>-0.013956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>-0.016682</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.012499</td>\n",
       "      <td>-0.001508</td>\n",
       "      <td>-0.010226</td>\n",
       "      <td>-0.008283</td>\n",
       "      <td>-0.010817</td>\n",
       "      <td>-0.016586</td>\n",
       "      <td>0.012493</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009826</td>\n",
       "      <td>0.011738</td>\n",
       "      <td>0.015933</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>-0.010980</td>\n",
       "      <td>-0.016145</td>\n",
       "      <td>-0.005911</td>\n",
       "      <td>0.015439</td>\n",
       "      <td>0.002698</td>\n",
       "      <td>0.006789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>-0.011723</td>\n",
       "      <td>0.005889</td>\n",
       "      <td>0.014623</td>\n",
       "      <td>0.004107</td>\n",
       "      <td>-0.016702</td>\n",
       "      <td>0.007496</td>\n",
       "      <td>0.012439</td>\n",
       "      <td>-0.007896</td>\n",
       "      <td>-0.008779</td>\n",
       "      <td>-0.012284</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017323</td>\n",
       "      <td>0.011574</td>\n",
       "      <td>0.006277</td>\n",
       "      <td>0.012388</td>\n",
       "      <td>0.011059</td>\n",
       "      <td>-0.014615</td>\n",
       "      <td>-0.000458</td>\n",
       "      <td>0.003131</td>\n",
       "      <td>0.005708</td>\n",
       "      <td>-0.008589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.000778</td>\n",
       "      <td>-0.008487</td>\n",
       "      <td>-0.017111</td>\n",
       "      <td>-0.010645</td>\n",
       "      <td>-0.016070</td>\n",
       "      <td>0.016308</td>\n",
       "      <td>-0.010574</td>\n",
       "      <td>0.014263</td>\n",
       "      <td>-0.000219</td>\n",
       "      <td>0.010294</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008487</td>\n",
       "      <td>0.010445</td>\n",
       "      <td>0.011382</td>\n",
       "      <td>-0.016198</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.005739</td>\n",
       "      <td>-0.007169</td>\n",
       "      <td>0.014873</td>\n",
       "      <td>0.003641</td>\n",
       "      <td>-0.002408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 18875 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Cluster 0  Cluster 2  Cluster 4  Cluster 6  Cluster 9  Cluster 10  \\\n",
       "0    -0.011237  -0.007775  -0.003946   0.011932  -0.013293    0.004583   \n",
       "1    -0.003501  -0.012792   0.008968   0.003467   0.006941   -0.001278   \n",
       "2    -0.003852   0.006655   0.014301  -0.005365   0.012869    0.009780   \n",
       "3    -0.005686   0.016590  -0.002658   0.004458   0.014662    0.001253   \n",
       "4     0.015104   0.015619   0.004610   0.010777  -0.000738   -0.008183   \n",
       "..         ...        ...        ...        ...        ...         ...   \n",
       "195  -0.000986  -0.013102  -0.015427  -0.014379   0.004253    0.002515   \n",
       "196  -0.004005  -0.000765   0.015550   0.016883  -0.014563    0.008051   \n",
       "197  -0.016682   0.000418   0.012499  -0.001508  -0.010226   -0.008283   \n",
       "198  -0.011723   0.005889   0.014623   0.004107  -0.016702    0.007496   \n",
       "199   0.000778  -0.008487  -0.017111  -0.010645  -0.016070    0.016308   \n",
       "\n",
       "     Cluster 15  Cluster 17  Cluster 18  Cluster 19  ...  Cluster 90902  \\\n",
       "0     -0.013791    0.007427   -0.013434    0.000388  ...      -0.007301   \n",
       "1      0.014176    0.026523   -0.008823    0.009218  ...       0.011292   \n",
       "2     -0.012053   -0.006668    0.005272   -0.008114  ...       0.003413   \n",
       "3      0.012888    0.026360   -0.008801   -0.011197  ...      -0.015570   \n",
       "4      0.012814    0.016787   -0.007946    0.004569  ...       0.017268   \n",
       "..          ...         ...         ...         ...  ...            ...   \n",
       "195   -0.016599   -0.014148   -0.009423   -0.016268  ...      -0.009657   \n",
       "196    0.012932   -0.012264   -0.013333   -0.005651  ...       0.000615   \n",
       "197   -0.010817   -0.016586    0.012493    0.001027  ...      -0.009826   \n",
       "198    0.012439   -0.007896   -0.008779   -0.012284  ...      -0.017323   \n",
       "199   -0.010574    0.014263   -0.000219    0.010294  ...      -0.008487   \n",
       "\n",
       "     Cluster 90903  Cluster 90906  Cluster 90925  Cluster 90929  \\\n",
       "0         0.001219       0.004761       0.001485      -0.011789   \n",
       "1         0.006556      -0.010139      -0.008745      -0.008215   \n",
       "2        -0.012003      -0.012839       0.015831       0.015738   \n",
       "3        -0.010009       0.011598       0.003125      -0.008722   \n",
       "4        -0.007491      -0.009670       0.007494      -0.004423   \n",
       "..             ...            ...            ...            ...   \n",
       "195       0.017060       0.013498      -0.006362       0.001687   \n",
       "196      -0.010362       0.001586      -0.017182       0.005578   \n",
       "197       0.011738       0.015933       0.000035      -0.010980   \n",
       "198       0.011574       0.006277       0.012388       0.011059   \n",
       "199       0.010445       0.011382      -0.016198       0.000682   \n",
       "\n",
       "     Cluster 90932  Cluster 90934  Cluster 90936  Cluster 90937  Cluster 90938  \n",
       "0         0.011120       0.015102       0.000324      -0.017233       0.014957  \n",
       "1        -0.006803      -0.014237      -0.013986      -0.002170      -0.010982  \n",
       "2        -0.008493       0.007017       0.005964       0.010606       0.009677  \n",
       "3         0.000452      -0.014814      -0.010106       0.015868      -0.007455  \n",
       "4        -0.005728      -0.000304      -0.015757      -0.008761       0.011665  \n",
       "..             ...            ...            ...            ...            ...  \n",
       "195      -0.008498       0.012576      -0.017683       0.011611      -0.004294  \n",
       "196      -0.014994      -0.002948      -0.005707      -0.009020      -0.013956  \n",
       "197      -0.016145      -0.005911       0.015439       0.002698       0.006789  \n",
       "198      -0.014615      -0.000458       0.003131       0.005708      -0.008589  \n",
       "199       0.005739      -0.007169       0.014873       0.003641      -0.002408  \n",
       "\n",
       "[200 rows x 18875 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make W[0] a dataframe\n",
    "W_df = pd.DataFrame(W[0])\n",
    "W_df = W_df.T\n",
    "W_df.columns = features\n",
    "W_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "n_models = 200\n",
    "\n",
    "\n",
    "models=[]\n",
    "for i in range(n_models):\n",
    "    X_boot, y_boot = resample(X, y, random_state=i)\n",
    "    model=SGDClassifier(loss='hinge', penalty= 'l1', max_iter=1000, tol=1e-3)\n",
    "    model.fit(X_boot, y_boot.ravel())\n",
    "    models.append(model)\n",
    "\n",
    "weights=np.zeros((n_models, X.shape[1]))\n",
    "for i in range(n_models):\n",
    "    weights[i]=models[i].coef_\n",
    "weights_df = pd.DataFrame(weights, columns=X_df.columns)\n",
    "\n",
    "corr_SVM = weights_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it in a csv\n",
    "corr_SVM.to_csv(f'../../data/temp/corr_SVM_{species}_{drug}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/temp/corr_SVM_Escherichia_coli_ciprofloxacin.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m corr_SVM \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../data/temp/corr_SVM_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mspecies\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdrug\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/temp/corr_SVM_Escherichia_coli_ciprofloxacin.csv'"
     ]
    }
   ],
   "source": [
    "corr_SVM = pd.read_csv(f'../../data/temp/corr_SVM_{species}_{drug}.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0.1 # -- UNSIGNED\n",
    "\n",
    "G=nx.Graph()\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "\n",
    "    for j in range(i+1, X.shape[1]):\n",
    "        node_i=X_df.columns[i]; node_j=X_df.columns[j]\n",
    "        correlation = corr_SVM.iloc[i, j]\n",
    "        if correlation > t:\n",
    "            G.add_edge(node_i, node_j, weight=correlation)\n",
    "\n",
    "# G=set_cluster_attributes(G)\n",
    "# nx.set_node_attributes(G, log_odds_dict, 'log_odds')\n",
    "\n",
    "nx.write_graphml(G, f'../../data/nets/{species}/{n_models}_randomized_SVM_{t}unsigned_corr_{drug}.graphml')\n",
    "\n",
    "edges_list = list(G.edges)\n",
    "edge_weights = [G[u][v]['weight'] for u, v in edges_list]\n",
    "print(f\"E: {len(edges_list)}, V: {G.number_of_nodes()} when {n_models} models are used, with corr threshold={t}; species={species}, drug={drug}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.write_graphml(G, f'../../data/nets/{species}/{n_models}_randomized_SVM_{t}unsigned_corr_{drug}.graphml')\n",
    "\n",
    "# edges_list = list(G.edges)\n",
    "# edge_weights = [G[u][v]['weight'] for u, v in edges_list]\n",
    "# print(f\"E: {len(edges_list)}, V: {G.number_of_nodes()} when {n_models} models are used, with corr threshold={t}; species={species}, drug={drug}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.read_graphml('../../data/nets/Escherichia_coli/200_randomized_SVM_0.2unsigned_corr_trimethoprim.graphml')\n",
    "edge_list = list(G.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "X_pairs = [(i.replace(' ', '_'), j.replace(' ', '_')) for i, j in edge_list]\n",
    "interaction_labeled_matrix = labeled_matrix.rename(columns={col: col.replace(' ', '_') for col in labeled_matrix.columns})\n",
    "\n",
    "# formula = f'SIR ~ {a} + {b} + {a}:{b}'\n",
    "\n",
    "test_pairs = X_pairs[:1]\n",
    "\n",
    "for pair in test_pairs:\n",
    "    a, b = pair\n",
    "    print(\" --- looking into the edge between\", a, \"and\", b)\n",
    "\n",
    "    results = smf.logit(formula=f'SIR ~ {a} + {b} + {a}:{b}', data=interaction_labeled_matrix).fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pairs=edge_list\n",
    "error_test=[(\"Cluster 0\", \"Cluster 20\")]\n",
    "\n",
    "p_values_list = []\n",
    "\n",
    "for pair in error_test:\n",
    "    a, b = pair\n",
    "    print(\" --- looking into the edge between\", a, \"and\", b)\n",
    "\n",
    "    X_selected = X_df.loc[:, [a, b]]\n",
    "\n",
    "    print(X_selected.shape)\n",
    "\n",
    "    X_selected = sm.add_constant(X_selected)\n",
    "\n",
    "    X_selected['interaction'] = X_selected.iloc[:, a] * X_selected.iloc[:, b]\n",
    "\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            logit_model = sm.Logit(y, X_selected)\n",
    "            result = logit_model.fit()\n",
    "            p_values_list.append(result.pvalues['interaction'])\n",
    "\n",
    "    except:\n",
    "        print('singular matrix; deleted')\n",
    "\n",
    "\n",
    "# reject, corrected_p_values= multipletests(p_values_list, method='fdr_bh')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = X_df.loc[:, [\"Cluster 0\", \"Cluster 20\"]]\n",
    "# -- check if the 2 cols are identical\n",
    "test = sm.add_constant(test)\n",
    "\n",
    "    # if X_selected.shape[1]==2:\n",
    "    #     X_selected['interaction'] = X_selected.iloc[:, 1] * X_selected.iloc[:, 0]\n",
    "    # X_selected['interaction'] = X_selected.iloc[:, 1] * X_selected.iloc[:, 2]\n",
    "\n",
    "test['interaction']=test.loc[:,\"Cluster 0\"] * test.loc[:,\"Cluster 20\"]\n",
    "test\n",
    "\n",
    "# X_selected\n",
    "# get corr\n",
    "# X_selected.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "np.random.seed(111)\n",
    "df = pd.DataFrame(np.random.binomial(1,0.5,(50,3)),columns=['x1','x2','y'])\n",
    "\n",
    "res1 = smf.logit(formula='y ~ x1 + x2 + x1:x2', data=df).fit()\n",
    "\n",
    "res1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smf.logit(formula='y ~ x1 + x2 + x1:x2', data=df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0; insig=0\n",
    "save_edges=[]\n",
    "p_values_list=[]\n",
    "\n",
    "for pair in edges_list:\n",
    "    a, b = pair\n",
    "    print(\" --- looking into the edge between\", a, \"and\", b)\n",
    "\n",
    "    X_selected = X_df.loc[:, [a, b]]\n",
    "\n",
    "    print(X_selected.shape)\n",
    "\n",
    "    X_selected = sm.add_constant(X_selected)\n",
    "    print(X_selected.shape)\n",
    "    if X_selected.shape[1]==2:\n",
    "        X_selected['interaction'] = X_selected.iloc[:, 1] * X_selected.iloc[:, 0]\n",
    "    X_selected['interaction'] = X_selected.iloc[:, 1] * X_selected.iloc[:, 2]\n",
    "\n",
    "    print(X_selected.shape)\n",
    "    \n",
    "    col=False\n",
    "\n",
    "\n",
    "    #account for collinearity\n",
    "    # --- matrix way\n",
    "    if np.linalg.matrix_rank(X_selected) < X_selected.shape[1]:\n",
    "        print('collinearity; deleted')\n",
    "        # G.remove_edge(*pair)\n",
    "        continue\n",
    "\n",
    "    # --- paper way\n",
    "    # if (X_selected['interaction']==X_selected[a]).all() or (X_selected['interaction']==X_selected[b]).all():      \n",
    "    #     col=True  \n",
    "    #     print(f'colinearity between interaction and one of the features {a} or {b}; deleted')\n",
    "    # elif (X_selected[b]==X_selected[a]).all():\n",
    "    #     col=True\n",
    "    #     print(f'colinearity between {a} and {b}; deleted')\n",
    "    # elif (X_selected['const']==X_selected[a]).all() or (X_selected['const']==X_selected[b]).all() or (X_selected['const']==X_selected['interaction']).all():\n",
    "    #     col=True\n",
    "    #     print('colinearity between one of the features and the constant term?!')\n",
    "\n",
    "    # if col:\n",
    "    #     G.remove_edge(*pair)\n",
    "    #     continue\n",
    "\n",
    "    # --- logistic regression\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            logit_model = sm.Logit(y, X_selected)\n",
    "            result = logit_model.fit()\n",
    "    except:\n",
    "        print('singular matrix; deleted')\n",
    "\n",
    "    p_values_list.append(p_values['interaction'])\n",
    "\n",
    "    # coefficients = result.params\n",
    "    p_values = result.pvalues\n",
    "\n",
    "    if p_values['interaction']<0.05:\n",
    "        print('not significant; deleted')\n",
    "        # G.remove_edge(*pair)\n",
    "        insig+=1\n",
    "    else:\n",
    "        count+=1\n",
    "        save_edges.append(pair)\n",
    "\n",
    "\n",
    "    # --- multiple testing Benjamini-Hochberg correction\n",
    "    reject, corrected_p_values= multipletests(p_values_list, method='fdr_bh')\n",
    "\n",
    "    for i, pair in enumerate(edges_list):\n",
    "        if corrected_p_values[i] < 0.05:\n",
    "            print(f'Edge {pair} is not significant; deleted')\n",
    "            G.remove_edge(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-way ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_2way_ANOVA(X_pairs, labeled_matrix):\n",
    "    '''\n",
    "    takes a list of tuples (pairs) - supposed to be pairs of features found in labeled_matrix - and a labeled matrix (X + y)\n",
    "    returns teh significant interaction (those having corr p_val<0.05) \n",
    "    accounting for multiple testing, using the Benjamini-Hochberg FDR correction\n",
    "\n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pairs = [(i.replace(' ', '_'), j.replace(' ', '_')) for i, j in edge_list]\n",
    "interaction_labeled_matrix = labeled_matrix.rename(columns={col: col.replace(' ', '_') for col in labeled_matrix.columns})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_list = list(G.edges)\n",
    "edge_weights = [G[u][v]['weight'] for u, v in edges_list]\n",
    "print(f\"E: {len(edges_list)}, V: {G.number_of_nodes()} when {n_models} models are used, with corr threshold={t}; species={species}, drug={drug}.\")\n",
    "\n",
    "save_edges=[]\n",
    "\n",
    "edges_list_2=[(i.replace(' ', '_'), j.replace(' ', '_')) for i, j in edges_list]\n",
    "labeled_matrix_2 = labeled_matrix.rename(columns={col: col.replace(' ', '_') for col in labeled_matrix.columns})\n",
    "p_values={}\n",
    "\n",
    "count_nan=0\n",
    "save_nan_testing=[]\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for pair in edges_list_2:\n",
    "        a, b = pair\n",
    "        # print(\" --- looking into the interaction between\", a, \"and\", b)\n",
    "        formula = f'SIR ~ {a} + {b} + {a}:{b}'\n",
    "        model = ols(formula, data=labeled_matrix_2).fit()\n",
    "        anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "        if not np.isnan(anova_table['PR(>F)'][2]):\n",
    "            p_values[anova_table.index[2]]=anova_table['PR(>F)'][2]\n",
    "        else:\n",
    "            save_nan_testing.append(pair)\n",
    "            count_nan+=1\n",
    "        \n",
    "print(f\"{count_nan} interactions have nan p-values - removed\")\n",
    "\n",
    "# mst correction - using fdr\n",
    "\n",
    "gene_gene_pair = list(p_values.keys())\n",
    "p_values_list = list(p_values.values())\n",
    "\n",
    "reject, corrected_p_values= fdrcorrection(p_values_list)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'gene_gene': gene_gene_pair,\n",
    "    'p-value': p_values_list,\n",
    "    'corr_p_value': corrected_p_values,\n",
    "    'reject_H0': reject\n",
    "})\n",
    "\n",
    "count_rejected=0\n",
    "for i in reject:\n",
    "    if i:\n",
    "        count_rejected+=1\n",
    "\n",
    "print(f\"Number of rejected H0: {count_rejected} out of {len(reject)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOR of co-occurence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clstr_df= get_cluster_representatives(f'../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}.fasta.clstr')\n",
    "_product_df=get_representative_products(f'../../pangenome-repo/Pangenome-Analysis-Workflow/codes/{species}/{species}.fasta')\n",
    "product_df = combine_cluster_product(clstr_df, _product_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  left joing X_df and product_df on the index\n",
    "X_t= X_df.T\n",
    "test = X_t.join(product_df, how='left')\n",
    "l=test['product_name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Cluster'] = test.index\n",
    "test.index = test['product_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_product_df.index= _product_df['product_name']\n",
    "p_ =_product_df.drop('product_name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner join test and _product_df on the index\n",
    "test = test.join(_product_df, how='inner', lsuffix='_caller', rsuffix='_other')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['product_name'].to_list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
